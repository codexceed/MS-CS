{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ea90c3",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning Systems : HW 2\n",
    "## Problem 1: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcb4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49e9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.07913953,  0.27972466, -1.        ],\n",
       "       [ 0.64158628,  0.73117014, -1.        ],\n",
       "       [ 0.69398778,  0.47006412,  1.        ],\n",
       "       [ 0.0713409 ,  0.0560696 ,  1.        ],\n",
       "       [ 0.14439238,  0.15546489, -1.        ],\n",
       "       [ 0.90697584,  0.88839703,  1.        ],\n",
       "       [ 0.24572304,  0.49236102, -1.        ],\n",
       "       [ 0.11568971,  0.88261834, -1.        ],\n",
       "       [ 0.36278988,  0.63100913, -1.        ],\n",
       "       [ 0.78673499,  0.81975189, -1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate training data\n",
    "def generate_data(n):\n",
    "    x = default_rng().uniform(low=0.0, high=1.0, size=(n, 2))\n",
    "    func = lambda x: 1 if x else -1\n",
    "    y = np.vectorize(func)(x[:, 0] > x[:, 1])\n",
    "    \n",
    "    return x, y\n",
    "X_train, y_train = generate_data(10)\n",
    "print(\"Training Data\")\n",
    "np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20538a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(X_train, y_train, epoch_lim=1000, hinge_loss=False):\n",
    "    \"\"\"Train weights for given data via perceptron algorithm.\n",
    "\n",
    "    Args:\n",
    "        X_train: Feature matrix\n",
    "        y_train: Class labels\n",
    "        epoch_lim: Upper epoch limit to cut-off training\n",
    "        hinge_loss: Specify whether to use hinge loss\n",
    "\n",
    "    Returns:\n",
    "        Trained weights\n",
    "    \"\"\"\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    epoch, convergence = 0, False\n",
    "    threshold = 1 if hinge_loss else 0\n",
    "\n",
    "    while not convergence and epoch < epoch_lim:\n",
    "        convergence = True\n",
    "        for i in range(len(X_train)):\n",
    "            x = X_train[i]\n",
    "            y = y_train[i]\n",
    "\n",
    "            margin = y * np.dot(w, x)\n",
    "            if margin <= threshold:\n",
    "                convergence = False\n",
    "                w += y * x\n",
    "\n",
    "        epoch += 1\n",
    "    \n",
    "    if not convergence:\n",
    "        print(\"Convergence not achieved. Too many epochs.\")\n",
    "    return w\n",
    "\n",
    "def predict_perceptron(x, weights):\n",
    "    \"\"\"Predict values using perceptron weights\n",
    "\n",
    "    Args:\n",
    "        x: Input features\n",
    "        weights: Perceptron weights\n",
    "\n",
    "    Return:\n",
    "        Output vector\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sign(np.dot(x, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c232428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence not achieved. Too many epochs.\n",
      "Mean accuracy with perceptron loss: 0.9911999999999999\n",
      "Mean accuracy with hinge loss: 0.99624\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "w_perceptron = train_perceptron(X_train, y_train)\n",
    "w_hinge = train_perceptron(X_train, y_train, hinge_loss=True)\n",
    "\n",
    "# Test\n",
    "perceptron_accuracy, hinge_accuracy = [], []\n",
    "for i in range(10):\n",
    "    X_test, y_test = generate_data(5000)\n",
    "    pred_perceptron = predict_perceptron(X_test, w_perceptron)\n",
    "    pred_hinge = predict_perceptron(X_test, w_hinge)\n",
    "    \n",
    "    perceptron_accuracy.append(sum(pred_perceptron == y_test) / pred_perceptron.shape[0])\n",
    "    hinge_accuracy.append(sum(pred_hinge == y_test) / pred_hinge.shape[0])\n",
    "\n",
    "print(f\"Mean accuracy with perceptron loss: {np.mean(perceptron_accuracy)}\")\n",
    "print(f\"Mean accuracy with hinge loss: {np.mean(hinge_accuracy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7087c",
   "metadata": {},
   "source": [
    "## Problem 2: Weight Initialization, Dead Neurons, Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bddd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepreplay.callbacks import ReplayData\n",
    "# from deepreplay.replay import Replay\n",
    "# from deepreplay.plot import compose_plots\n",
    "# from keras.initializers import normal\n",
    "# from matplotlib import pyplot as plt\n",
    "# from q2.model_builder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c959c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepreplay.datasets.ball import load_data\n",
    "\n",
    "# X, y = load_data(n_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b4133d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'part2_weight_initializers.h5'\n",
    "# group_name = 'sigmoid_stdev_0.01'\n",
    "\n",
    "# # Uses normal initializer\n",
    "# initializer = normal(mean=0, stddev=0.01, seed=13)\n",
    "\n",
    "# # Builds BLOCK model\n",
    "# model = build_model(n_layers=5, input_dim=10, units=100, \n",
    "#                     activation='sigmoid', initializer=initializer)\n",
    "\n",
    "# # Since we only need initial weights, we don't even need to train the model! \n",
    "# # We still use the ReplayData callback, but we can pass the model as argument instead\n",
    "# replaydata = ReplayData(X, y, filename=filename, group_name=group_name, model=model)\n",
    "\n",
    "# # Now we feed the data to the actual Replay object\n",
    "# # so we can build the visualizations\n",
    "# replay = Replay(replay_filename=filename, group_name=group_name)\n",
    "\n",
    "# # Using subplot2grid to assemble a complex figure...\n",
    "# fig = plt.figure(figsize=(12, 6))\n",
    "# ax_zvalues = plt.subplot2grid((2, 2), (0, 0))\n",
    "# ax_weights = plt.subplot2grid((2, 2), (0, 1))\n",
    "# ax_activations = plt.subplot2grid((2, 2), (1, 0))\n",
    "# ax_gradients = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "# wv = replay.build_weights(ax_weights)\n",
    "# gv = replay.build_gradients(ax_gradients)\n",
    "# # Z-values\n",
    "# zv = replay.build_outputs(ax_zvalues, before_activation=True, \n",
    "#                           exclude_outputs=True, include_inputs=False)\n",
    "# # Activations\n",
    "# av = replay.build_outputs(ax_activations, exclude_outputs=True, include_inputs=False)\n",
    "\n",
    "# # Finally, we use compose_plots to update all\n",
    "# # visualizations at once\n",
    "# fig = compose_plots([zv, wv, av, gv], \n",
    "#                     epoch=0, \n",
    "#                     title=r'Activation: sigmoid - Initializer: Normal $\\sigma = 0.01$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4092a",
   "metadata": {},
   "source": [
    "## Problem 3: Batch Norm, Dropout, MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072b8bb",
   "metadata": {},
   "source": [
    "### 1.\n",
    "#### Co-Adaptation\n",
    "In Neural network, co-adaptation means that some neurons are highly dependent on others. Conseuquently, these co-dependent units seem to output correctly only under a very narrow and specific set of circumstances (inputs and features), causing them to frequently fire erroneously under general conditions. Co-adapted neurons have a high tendency for inaccurate results since even a single incorrect input to a unit can trigger a chain of incorrect outputs in all of its co-adapted peers. This behaviour is akin to overfitting, where the model performs great only on the training data (fixed specific circumstance), but fails to generalize and therefore performs poorly on validation/test data.\n",
    "\n",
    "#### Internal Covariate Shift\n",
    "We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. It's well known that a neural network trains faster the more consistent the distribution of the features are in any given layer. A shift in the distribution causes overhead due to the excess iterations required for the network to adjust to the new distributions. We can achieve convergence faster if we employ methods to counter internal covariate shift as it helps avoid this overhead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49c364",
   "metadata": {},
   "source": [
    "### LeNet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f829b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0a236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Define relevant variables for the ML task\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "216ed773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6843a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the convolutional neural network\n",
    "def get_accuracy(pred_model, loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "            pred_model = pred_model.to(\"cpu\")\n",
    "            outputs = pred_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pred_model.to(device)\n",
    "        return 100 * correct / total\n",
    "    \n",
    "\n",
    "def get_mean_loss(pred_model, loader):\n",
    "    tot_loss = 0\n",
    "    for images, labels in loader:  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = pred_model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        tot_loss += loss\n",
    "    \n",
    "    return tot_loss / len(loader.dataset)\n",
    "        \n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes, input_tfs = [], hidden_tfs = []):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # Input Layer\n",
    "        self.conv1 =nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)\n",
    "        input_tfs = [nn.Identity()] + input_tfs\n",
    "        self.input_tfs = nn.Sequential(*input_tfs)\n",
    "        self.layer1 = nn.Sequential(nn.ReLU(), nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        hidden_tfs = [nn.Identity()] + hidden_tfs\n",
    "        self.hidden_tfs = nn.Sequential(*hidden_tfs)\n",
    "        self.layer2 = nn.Sequential(nn.ReLU(), nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.input_tfs(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.hidden_tfs(out)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f67d1a",
   "metadata": {},
   "source": [
    "### 2. Standard Norm on Input Layer, Batch Norm on Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ed7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes, hidden_tfs = [nn.BatchNorm2d(16)]).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef785d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [400/938], Loss: 0.0211\n",
      "Epoch [1/10], Step [800/938], Loss: 0.0345\n",
      "Epoch [2/10], Step [400/938], Loss: 0.0153\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0164\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0945\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0070\n",
      "Epoch [4/10], Step [400/938], Loss: 0.1194\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0186\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0149\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0151\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0203\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0168\n",
      "Epoch [7/10], Step [800/938], Loss: 0.1242\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0377\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0120\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0406\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0027\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0186\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "std_tr_losses, std_ts_losses, std_tr_accuracy, std_ts_accuracy = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        epoch_loss += loss * images.size(0)\n",
    "        \t\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \t\t\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    \n",
    "    std_tr_accuracy.append(get_accuracy(model, train_loader))\n",
    "    std_ts_accuracy.append(get_accuracy(model, test_loader))\n",
    "    std_tr_losses.append(epoch_loss / len(train_loader.dataset))\n",
    "    std_ts_losses.append(get_mean_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10ed2447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on train images: 99.78333333333333 %\n",
      "Accuracy of the network on test images: 98.57 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "print('Accuracy of the network on train images: {} %'.format(get_accuracy(model, train_loader)))\n",
    "\n",
    "print('Accuracy of the network on test images: {} %'.format(get_accuracy(model, test_loader)))\n",
    "\t "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918e1d6",
   "metadata": {},
   "source": [
    "#### Input Layer Standard Normalized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c21af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weights: {model.state_dict()['conv1.weight']}\")\n",
    "print(f\"Bias: {model.state_dict()['conv1.bias']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b00ebc",
   "metadata": {},
   "source": [
    "#### Hidden Later Batch Normalized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9630b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weights: {model.state_dict()['hidden_tfs.1.weight']}\")\n",
    "print(f\"Bias: {model.state_dict()['hidden_tfs.1.bias']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113466f",
   "metadata": {},
   "source": [
    "### 3. Batch Normalization on Input and Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24476e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes, input_tfs = [nn.BatchNorm2d(6)], hidden_tfs = [nn.BatchNorm2d(16)]).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "bn_tr_losses, bn_ts_losses, bn_tr_accuracy, bn_ts_accuracy = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        epoch_loss += loss * images.size(0)\n",
    "        \t\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \t\t\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    \n",
    "    bn_tr_accuracy.append(get_accuracy(model, train_loader))\n",
    "    bn_ts_accuracy.append(get_accuracy(model, test_loader))\n",
    "    bn_tr_losses.append(epoch_loss / len(train_loader.dataset))\n",
    "    bn_ts_losses.append(get_mean_loss(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "print('Accuracy of the network on train images: {} %'.format(accuracy(model, train_loader)))\n",
    "\n",
    "print('Accuracy of the network on test images: {} %'.format(accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "w_input, w_hidden = model.state_dict()[\"input_tfs.1.weight\"], model.state_dict()[\"hidden_tfs.1.weight\"]\n",
    "data = {\"Input Layer\": w_input.tolist(), \"Hidden Layer\": w_hidden.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input Layer Weights\")\n",
    "sns.violinplot(x=\"Input Layer\", data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c802a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hidden Layer Weights\")\n",
    "sns.violinplot(x=\"Hidden Layer\", data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f14d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "axes[0].plot([i + 1 for i in range(num_epochs)], std_tr_losses, label=\"Std Norm Training\")\n",
    "axes[0].plot([i + 1 for i in range(num_epochs)], bn_tr_losses, label=\"Batch Norm Training\")\n",
    "axes[0].plot([i + 1 for i in range(num_epochs)], std_ts_losses, label=\"Std Norm Test\")\n",
    "axes[0].plot([i + 1 for i in range(num_epochs)], bn_ts_losses, label=\"Batch Norm Test\")\n",
    "axes[0].legend(loc=\"best\")\n",
    "axes[0].title(\"Loss vs Epochs\")\n",
    "\n",
    "axes[1].plot([i + 1 for i in range(num_epochs)], std_tr_accuracy, label=\"Std Norm Training\")\n",
    "axes[1].plot([i + 1 for i in range(num_epochs)], bn_tr_accuracy, label=\"Batch Norm Training\")\n",
    "axes[1].plot([i + 1 for i in range(num_epochs)], std_ts_accuracy, label=\"Std Norm Test\")\n",
    "axes[1].plot([i + 1 for i in range(num_epochs)], bn_ts_accuracy, label=\"Batch Norm Test\")\n",
    "axes[1].legend(loc=\"best\")\n",
    "axes[1].title(\"Accuracy vs Epochs\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
