{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ea90c3",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning Systems : HW 2\n",
    "## Problem 1: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcb4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e49e9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.77630199,  0.67073049,  1.        ],\n       [ 0.54433576,  0.95879851, -1.        ],\n       [ 0.03631605,  0.11214203, -1.        ],\n       [ 0.16496294,  0.00253814,  1.        ],\n       [ 0.73906551,  0.88131042, -1.        ],\n       [ 0.48309446,  0.53066175, -1.        ],\n       [ 0.7533649 ,  0.74075462,  1.        ],\n       [ 0.48779942,  0.79262143, -1.        ],\n       [ 0.85317393,  0.99522347, -1.        ],\n       [ 0.22314569,  0.09628791,  1.        ]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate training data\n",
    "def generate_data(n):\n",
    "    x = default_rng().uniform(low=0.0, high=1.0, size=(n, 2))\n",
    "    func = lambda x: 1 if x else -1\n",
    "    y = np.vectorize(func)(x[:, 0] > x[:, 1])\n",
    "    \n",
    "    return x, y\n",
    "X_train, y_train = generate_data(10)\n",
    "print(\"Training Data\")\n",
    "np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20538a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(X_train, y_train, epoch_lim=1000, hinge_loss=False):\n",
    "    \"\"\"Train weights for given data via perceptron algorithm.\n",
    "\n",
    "    Args:\n",
    "        X_train: Feature matrix\n",
    "        y_train: Class labels\n",
    "        epoch_lim: Upper epoch limit to cut-off training\n",
    "        hinge_loss: Specify whether to use hinge loss\n",
    "\n",
    "    Returns:\n",
    "        Trained weights\n",
    "    \"\"\"\n",
    "    w = np.zeros(X_train.shape[1])\n",
    "    epoch, convergence = 0, False\n",
    "    threshold = 1 if hinge_loss else 0\n",
    "\n",
    "    while not convergence and epoch < epoch_lim:\n",
    "        convergence = True\n",
    "        for i in range(len(X_train)):\n",
    "            x = X_train[i]\n",
    "            y = y_train[i]\n",
    "\n",
    "            margin = y * np.dot(w, x)\n",
    "            if margin <= threshold:\n",
    "                convergence = False\n",
    "                w += y * x\n",
    "\n",
    "        epoch += 1\n",
    "    \n",
    "    if not convergence:\n",
    "        print(\"Convergence not achieved. Too many epochs.\")\n",
    "    return w\n",
    "\n",
    "def predict_perceptron(x, weights):\n",
    "    \"\"\"Predict values using perceptron weights\n",
    "\n",
    "    Args:\n",
    "        x: Input features\n",
    "        weights: Perceptron weights\n",
    "\n",
    "    Return:\n",
    "        Output vector\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sign(np.dot(x, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c232428b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy with perceptron loss: 0.96868\n",
      "Mean accuracy with hinge loss: 0.98608\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "w_perceptron = train_perceptron(X_train, y_train)\n",
    "w_hinge = train_perceptron(X_train, y_train, hinge_loss=True)\n",
    "\n",
    "# Test\n",
    "perceptron_accuracy, hinge_accuracy = [], []\n",
    "for i in range(10):\n",
    "    X_test, y_test = generate_data(5000)\n",
    "    pred_perceptron = predict_perceptron(X_test, w_perceptron)\n",
    "    pred_hinge = predict_perceptron(X_test, w_hinge)\n",
    "    \n",
    "    perceptron_accuracy.append(sum(pred_perceptron == y_test) / pred_perceptron.shape[0])\n",
    "    hinge_accuracy.append(sum(pred_hinge == y_test) / pred_hinge.shape[0])\n",
    "\n",
    "print(f\"Mean accuracy with perceptron loss: {np.mean(perceptron_accuracy)}\")\n",
    "print(f\"Mean accuracy with hinge loss: {np.mean(hinge_accuracy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7087c",
   "metadata": {},
   "source": [
    "## Problem 2: Weight Initialization, Dead Neurons, Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bddd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepreplay.callbacks import ReplayData\n",
    "from deepreplay.replay import Replay\n",
    "from deepreplay.plot import compose_plots\n",
    "from keras.initializers import normal\n",
    "from matplotlib import pyplot as plt\n",
    "from q2.model_builder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c959c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepreplay.datasets.ball import load_data\n",
    "\n",
    "X, y = load_data(n_dims=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4133d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to synchronously create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m build_model(n_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, input_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, units\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, \n\u001B[1;32m     10\u001B[0m                     activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m, initializer\u001B[38;5;241m=\u001B[39minitializer)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Since we only need initial weights, we don't even need to train the model! \u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# We still use the ReplayData callback, but we can pass the model as argument instead\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m replaydata \u001B[38;5;241m=\u001B[39m \u001B[43mReplayData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroup_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Now we feed the data to the actual Replay object\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# so we can build the visualizations\u001B[39;00m\n\u001B[1;32m     18\u001B[0m replay \u001B[38;5;241m=\u001B[39m Replay(replay_filename\u001B[38;5;241m=\u001B[39mfilename, group_name\u001B[38;5;241m=\u001B[39mgroup_name)\n",
      "File \u001B[0;32m~/Repos/MS-CS/Courses/Intro to Deep Learning Systems/py3.9_env/lib/python3.9/site-packages/deepreplay/callbacks.py:62\u001B[0m, in \u001B[0;36mReplayData.__init__\u001B[0;34m(self, inputs, targets, filename, group_name, model)\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_params({\n\u001B[1;32m     57\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs),\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs),\n\u001B[1;32m     60\u001B[0m     })\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_name \u001B[38;5;241m=\u001B[39m group_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_init\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_begin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_name \u001B[38;5;241m=\u001B[39m group_name\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/Repos/MS-CS/Courses/Intro to Deep Learning Systems/py3.9_env/lib/python3.9/site-packages/deepreplay/callbacks.py:112\u001B[0m, in \u001B[0;36mReplayData.on_train_begin\u001B[0;34m(self, logs)\u001B[0m\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callable(metric):\n\u001B[1;32m    111\u001B[0m         metric_name \u001B[38;5;241m=\u001B[39m metric\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(metric, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m metric\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m--> 112\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetric_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup\u001B[38;5;241m.\u001B[39mcreate_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m, shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_epochs,), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mlayers):\n",
      "File \u001B[0;32m~/Repos/MS-CS/Courses/Intro to Deep Learning Systems/py3.9_env/lib/python3.9/site-packages/h5py/_hl/group.py:149\u001B[0m, in \u001B[0;36mGroup.create_dataset\u001B[0;34m(self, name, shape, dtype, data, **kwds)\u001B[0m\n\u001B[1;32m    146\u001B[0m         parent_path, name \u001B[38;5;241m=\u001B[39m name\u001B[38;5;241m.\u001B[39mrsplit(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    147\u001B[0m         group \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequire_group(parent_path)\n\u001B[0;32m--> 149\u001B[0m dsid \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_new_dset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m dset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mDataset(dsid)\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dset\n",
      "File \u001B[0;32m~/Repos/MS-CS/Courses/Intro to Deep Learning Systems/py3.9_env/lib/python3.9/site-packages/h5py/_hl/dataset.py:142\u001B[0m, in \u001B[0;36mmake_new_dset\u001B[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    139\u001B[0m     sid \u001B[38;5;241m=\u001B[39m h5s\u001B[38;5;241m.\u001B[39mcreate_simple(shape, maxshape)\n\u001B[0;32m--> 142\u001B[0m dset_id \u001B[38;5;241m=\u001B[39m \u001B[43mh5d\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdcpl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdcpl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, Empty)):\n\u001B[1;32m    145\u001B[0m     dset_id\u001B[38;5;241m.\u001B[39mwrite(h5s\u001B[38;5;241m.\u001B[39mALL, h5s\u001B[38;5;241m.\u001B[39mALL, data)\n",
      "File \u001B[0;32mh5py/_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mh5py/h5d.pyx:87\u001B[0m, in \u001B[0;36mh5py.h5d.create\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Unable to synchronously create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "filename = 'part2_weight_initializers.h5'\n",
    "group_name = 'sigmoid_stdev_0.01'\n",
    "\n",
    "# Uses normal initializer\n",
    "initializer = normal(mean=0, stddev=0.01, seed=13)\n",
    "\n",
    "# Builds BLOCK model\n",
    "model = build_model(n_layers=5, input_dim=10, units=100, \n",
    "                    activation='sigmoid', initializer=initializer)\n",
    "\n",
    "# Since we only need initial weights, we don't even need to train the model! \n",
    "# We still use the ReplayData callback, but we can pass the model as argument instead\n",
    "replaydata = ReplayData(X, y, filename=filename, group_name=group_name, model=model)\n",
    "\n",
    "# Now we feed the data to the actual Replay object\n",
    "# so we can build the visualizations\n",
    "replay = Replay(replay_filename=filename, group_name=group_name)\n",
    "\n",
    "# Using subplot2grid to assemble a complex figure...\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax_zvalues = plt.subplot2grid((2, 2), (0, 0))\n",
    "ax_weights = plt.subplot2grid((2, 2), (0, 1))\n",
    "ax_activations = plt.subplot2grid((2, 2), (1, 0))\n",
    "ax_gradients = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "wv = replay.build_weights(ax_weights)\n",
    "gv = replay.build_gradients(ax_gradients)\n",
    "# Z-values\n",
    "zv = replay.build_outputs(ax_zvalues, before_activation=True, \n",
    "                          exclude_outputs=True, include_inputs=False)\n",
    "# Activations\n",
    "av = replay.build_outputs(ax_activations, exclude_outputs=True, include_inputs=False)\n",
    "\n",
    "# Finally, we use compose_plots to update all\n",
    "# visualizations at once\n",
    "fig = compose_plots([zv, wv, av, gv], \n",
    "                    epoch=0, \n",
    "                    title=r'Activation: sigmoid - Initializer: Normal $\\sigma = 0.01$')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}