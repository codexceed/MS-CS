{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1_2_submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN5lM2csj9v7"
      },
      "source": [
        "# Computer Vision CSCI-GA.2272-001 Assignment 1, part 2.\n",
        "\n",
        "Fall 2021 semester.\n",
        "\n",
        "Due date: **September 30th 2021.**\n",
        "\n",
        "## Introduction\n",
        "This assignment requires you to participate in a Kaggle competition with the rest of the class on the The German Traffic Sign Recognition Benchmark [http://benchmark.ini.rub.de/?section=gtsrb&subsection=news]. The objective is to produce a model that gives the highest possible accuracy on the test portion of this dataset. You can register for the competition using the private link: https://www.kaggle.com/c/nyu-computer-vision-csci-ga2271-2021/overview.\n",
        "\n",
        "Skeleton code is provided in the colab below. This contains code for training a simple default model and evaluating it on the test set. The evaluation script produces a file gtsrb_kaggle.csv that lists the IDs of the test set images, along with their predicted label. This file should be uploaded to the Kaggle webpage, which will then produce a test accuracy score. \n",
        "\n",
        "Your goal is to implement a new model architecture that improves upon the baseline performance. You are free to implement any approach covered in class or from research papers. This part will count for 50% of the overall grade for assignment 1. This Grading will depend on your Kaggle performance and rank, as well as novelty of the architecture.  \n",
        "\n",
        "## Rules\n",
        "You should make a copy of this Colab (File->Save a copy in Drive). Please start the assignment early and don’t be afraid to ask for help from either the TAs or myself. You are allowed to collaborate with other students in terms discussing ideas and possible solutions. However you code up the solution yourself, i.e. you must write your own code. Copying your friends code and just changing all the names of the variables is not allowed! You are not allowed to use solutions from similar assignments in courses from other institutions, or those found elsewhere on the web.\n",
        "Your solutions should be submitted via the Brightspace system. This should include a brief description (in the Colab) explaining the model architectures you explored, citing any relevant papers or techniques that you used. You should also include convergence plots of training accuracy vs epoch for relevant models. \n",
        "\n",
        "### Important Details\n",
        "• You are only allowed eight (8) submissions to the Kaggle evaluation server per day. This is to prevent over-fitting on the test dataset. So be sure to start the assignment early!\n",
        "\n",
        "• You are NOT ALLOWED to use the test set labels during training in any way. Doing so will be regarded as cheating and penalized accordingly.\n",
        "\n",
        "• The evaluation metric is accuracy, i.e. the fraction of test set examples where the predicted label agrees with the ground truth label.\n",
        "\n",
        "• You should be able to achieve a test accuracy of at least 0.95. \n",
        "\n",
        "• *Extra important:* Please use your NYU NetID as your Kaggle username, so the TAs can figure out which user you are on the leaderboard. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUfyAJqvwptT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pghx7ngowha0"
      },
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "\n",
        "1.  Download `dataset.zip` from to your local machine.\n",
        "2.  Unzip the file. You should see a `dataset` directory with three subfolders (`training,validation,testing`). \n",
        "3.  Go to Google Drive (on your NYU account) and make a directory `assign2_dataset` (New button --> New Folder).\n",
        "4.  Upload each of the three subfolders to it (New button --> Folder upload). \n",
        "5.  Run the code block below. It will ask for permission to mount your Google Drive (NYU account) so this colab can access it. Paste the authorization code into the box as requested. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0aPnIKXpWbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193bbb84-48d4-49f7-d804-e48741fdf7a4"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd  /content/drive/'My Drive'/assign2_dataset/"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/assign2_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6jVfIVtrn5u"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z21UKj_bT--_"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size = 64\n",
        "momentum = 0.9\n",
        "lr = 0.0001\n",
        "epochs = 15\n",
        "log_interval = 10\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_path=\"X.pt\", y_path=\"y.pt\"):\n",
        "\n",
        "        self.X = torch.load(X_path).squeeze(1)\n",
        "        self.y = torch.load(y_path).squeeze(1)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = MyDataset(X_path=\"train/X.pt\", y_path=\"train/y.pt\")\n",
        "val_dataset = MyDataset(X_path=\"validation/X.pt\", y_path=\"validation/y.pt\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHEuyr8Rn_43"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# def matplotlib_imshow(img, one_channel=False):\n",
        "#     if one_channel:\n",
        "#         img = img.mean(dim=0)\n",
        "#     img = img / 2 + 0.5     # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "#     if one_channel:\n",
        "#         plt.imshow(npimg, cmap=\"Greys\")\n",
        "#     else:\n",
        "#         plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# for idx in range(20):\n",
        "#   matplotlib_imshow(train_dataset[idx][0])\n",
        "#   print(idx, train_dataset[idx][1])\n",
        "\n",
        "# idx = 12\n",
        "# matplotlib_imshow(train_dataset[idx][0])\n",
        "# print(idx, train_dataset[idx][1])"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd6W0pQRvZKO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeev4SoMvazV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nclasses = 43 # GTSRB has 43 classes\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 50, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(50, 200, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(200, 500, kernel_size=5)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(50)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(200)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(500)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(500, 50)\n",
        "        self.fc2 = nn.Linear(50, nclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(f\"Input Shape: {x.shape}\")\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = self.batch_norm1(x)\n",
        "        # print(f\"Conv1 Output Shape: {x.shape}\")\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = self.batch_norm2(x)\n",
        "        # print(f\"Conv2 Output Shape: {x.shape}\")\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.batch_norm3(x)\n",
        "        # print(f\"Conv3 Output Shape: {x.shape}\")\n",
        "        x = x.view(-1, 500)\n",
        "        # print(f\"Reshape: {x.shape}\")\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x,dim=1)\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty9TAvrdvf8C"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A5-OCgBvhXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf17ca2-f86a-4b25-9cbe-9269228343e4"
      },
      "source": [
        "model = Net().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    \n",
        "    training_accuracy = 100*correct/len(train_loader.dataset)\n",
        "    print(f\"Training accuracy: {training_accuracy}\")\n",
        "    return training_accuracy\n",
        "    \n",
        "\n",
        "def validation():\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = model(data)\n",
        "        validation_loss += F.nll_loss(output, target, reduction=\"sum\").item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "\n",
        "train_accuracy_points = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_accuracy_points.append(train(epoch))\n",
        "    validation()\n",
        "    model_file = 'model_' + str(epoch) + '.pth'\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    print('\\nSaved model to ' + model_file + '.')\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/35339 (0%)]\tLoss: 3.779272\n",
            "Train Epoch: 1 [640/35339 (2%)]\tLoss: 3.659363\n",
            "Train Epoch: 1 [1280/35339 (4%)]\tLoss: 3.585432\n",
            "Train Epoch: 1 [1920/35339 (5%)]\tLoss: 3.276061\n",
            "Train Epoch: 1 [2560/35339 (7%)]\tLoss: 3.410810\n",
            "Train Epoch: 1 [3200/35339 (9%)]\tLoss: 3.220715\n",
            "Train Epoch: 1 [3840/35339 (11%)]\tLoss: 3.059530\n",
            "Train Epoch: 1 [4480/35339 (13%)]\tLoss: 2.988729\n",
            "Train Epoch: 1 [5120/35339 (14%)]\tLoss: 3.003740\n",
            "Train Epoch: 1 [5760/35339 (16%)]\tLoss: 3.041141\n",
            "Train Epoch: 1 [6400/35339 (18%)]\tLoss: 2.994274\n",
            "Train Epoch: 1 [7040/35339 (20%)]\tLoss: 2.856915\n",
            "Train Epoch: 1 [7680/35339 (22%)]\tLoss: 2.900438\n",
            "Train Epoch: 1 [8320/35339 (24%)]\tLoss: 2.404506\n",
            "Train Epoch: 1 [8960/35339 (25%)]\tLoss: 2.355125\n",
            "Train Epoch: 1 [9600/35339 (27%)]\tLoss: 2.362021\n",
            "Train Epoch: 1 [10240/35339 (29%)]\tLoss: 2.321698\n",
            "Train Epoch: 1 [10880/35339 (31%)]\tLoss: 2.521634\n",
            "Train Epoch: 1 [11520/35339 (33%)]\tLoss: 2.304737\n",
            "Train Epoch: 1 [12160/35339 (34%)]\tLoss: 2.061462\n",
            "Train Epoch: 1 [12800/35339 (36%)]\tLoss: 1.886301\n",
            "Train Epoch: 1 [13440/35339 (38%)]\tLoss: 2.078664\n",
            "Train Epoch: 1 [14080/35339 (40%)]\tLoss: 1.951546\n",
            "Train Epoch: 1 [14720/35339 (42%)]\tLoss: 1.873667\n",
            "Train Epoch: 1 [15360/35339 (43%)]\tLoss: 1.647095\n",
            "Train Epoch: 1 [16000/35339 (45%)]\tLoss: 1.985287\n",
            "Train Epoch: 1 [16640/35339 (47%)]\tLoss: 1.841302\n",
            "Train Epoch: 1 [17280/35339 (49%)]\tLoss: 1.758513\n",
            "Train Epoch: 1 [17920/35339 (51%)]\tLoss: 1.962733\n",
            "Train Epoch: 1 [18560/35339 (52%)]\tLoss: 1.763862\n",
            "Train Epoch: 1 [19200/35339 (54%)]\tLoss: 1.697272\n",
            "Train Epoch: 1 [19840/35339 (56%)]\tLoss: 1.827724\n",
            "Train Epoch: 1 [20480/35339 (58%)]\tLoss: 1.773695\n",
            "Train Epoch: 1 [21120/35339 (60%)]\tLoss: 1.780788\n",
            "Train Epoch: 1 [21760/35339 (61%)]\tLoss: 1.736001\n",
            "Train Epoch: 1 [22400/35339 (63%)]\tLoss: 1.401063\n",
            "Train Epoch: 1 [23040/35339 (65%)]\tLoss: 1.362294\n",
            "Train Epoch: 1 [23680/35339 (67%)]\tLoss: 1.468410\n",
            "Train Epoch: 1 [24320/35339 (69%)]\tLoss: 1.374165\n",
            "Train Epoch: 1 [24960/35339 (71%)]\tLoss: 1.394575\n",
            "Train Epoch: 1 [25600/35339 (72%)]\tLoss: 1.660973\n",
            "Train Epoch: 1 [26240/35339 (74%)]\tLoss: 1.386040\n",
            "Train Epoch: 1 [26880/35339 (76%)]\tLoss: 1.625241\n",
            "Train Epoch: 1 [27520/35339 (78%)]\tLoss: 1.402739\n",
            "Train Epoch: 1 [28160/35339 (80%)]\tLoss: 1.303392\n",
            "Train Epoch: 1 [28800/35339 (81%)]\tLoss: 1.214664\n",
            "Train Epoch: 1 [29440/35339 (83%)]\tLoss: 1.350206\n",
            "Train Epoch: 1 [30080/35339 (85%)]\tLoss: 1.450188\n",
            "Train Epoch: 1 [30720/35339 (87%)]\tLoss: 1.087533\n",
            "Train Epoch: 1 [31360/35339 (89%)]\tLoss: 1.162735\n",
            "Train Epoch: 1 [32000/35339 (90%)]\tLoss: 1.235266\n",
            "Train Epoch: 1 [32640/35339 (92%)]\tLoss: 1.440732\n",
            "Train Epoch: 1 [33280/35339 (94%)]\tLoss: 1.100324\n",
            "Train Epoch: 1 [33920/35339 (96%)]\tLoss: 0.969651\n",
            "Train Epoch: 1 [34560/35339 (98%)]\tLoss: 1.182745\n",
            "Train Epoch: 1 [35200/35339 (99%)]\tLoss: 1.086103\n",
            "Training accuracy: 51.76150894165039\n",
            "\n",
            "Validation set: Average loss: 1.7658, Accuracy: 2286/3870 (59%)\n",
            "\n",
            "\n",
            "Saved model to model_1.pth.\n",
            "Train Epoch: 2 [0/35339 (0%)]\tLoss: 1.157504\n",
            "Train Epoch: 2 [640/35339 (2%)]\tLoss: 1.001372\n",
            "Train Epoch: 2 [1280/35339 (4%)]\tLoss: 1.071835\n",
            "Train Epoch: 2 [1920/35339 (5%)]\tLoss: 0.970113\n",
            "Train Epoch: 2 [2560/35339 (7%)]\tLoss: 1.173521\n",
            "Train Epoch: 2 [3200/35339 (9%)]\tLoss: 1.158691\n",
            "Train Epoch: 2 [3840/35339 (11%)]\tLoss: 0.965757\n",
            "Train Epoch: 2 [4480/35339 (13%)]\tLoss: 1.243875\n",
            "Train Epoch: 2 [5120/35339 (14%)]\tLoss: 1.164050\n",
            "Train Epoch: 2 [5760/35339 (16%)]\tLoss: 0.893139\n",
            "Train Epoch: 2 [6400/35339 (18%)]\tLoss: 0.747909\n",
            "Train Epoch: 2 [7040/35339 (20%)]\tLoss: 0.917765\n",
            "Train Epoch: 2 [7680/35339 (22%)]\tLoss: 0.808064\n",
            "Train Epoch: 2 [8320/35339 (24%)]\tLoss: 0.931067\n",
            "Train Epoch: 2 [8960/35339 (25%)]\tLoss: 1.038522\n",
            "Train Epoch: 2 [9600/35339 (27%)]\tLoss: 0.792361\n",
            "Train Epoch: 2 [10240/35339 (29%)]\tLoss: 0.863117\n",
            "Train Epoch: 2 [10880/35339 (31%)]\tLoss: 0.976902\n",
            "Train Epoch: 2 [11520/35339 (33%)]\tLoss: 1.046686\n",
            "Train Epoch: 2 [12160/35339 (34%)]\tLoss: 0.843814\n",
            "Train Epoch: 2 [12800/35339 (36%)]\tLoss: 0.895180\n",
            "Train Epoch: 2 [13440/35339 (38%)]\tLoss: 0.974125\n",
            "Train Epoch: 2 [14080/35339 (40%)]\tLoss: 0.724074\n",
            "Train Epoch: 2 [14720/35339 (42%)]\tLoss: 0.770765\n",
            "Train Epoch: 2 [15360/35339 (43%)]\tLoss: 0.986072\n",
            "Train Epoch: 2 [16000/35339 (45%)]\tLoss: 0.693236\n",
            "Train Epoch: 2 [16640/35339 (47%)]\tLoss: 0.915449\n",
            "Train Epoch: 2 [17280/35339 (49%)]\tLoss: 0.852378\n",
            "Train Epoch: 2 [17920/35339 (51%)]\tLoss: 0.710198\n",
            "Train Epoch: 2 [18560/35339 (52%)]\tLoss: 0.773011\n",
            "Train Epoch: 2 [19200/35339 (54%)]\tLoss: 0.706096\n",
            "Train Epoch: 2 [19840/35339 (56%)]\tLoss: 0.536229\n",
            "Train Epoch: 2 [20480/35339 (58%)]\tLoss: 0.819867\n",
            "Train Epoch: 2 [21120/35339 (60%)]\tLoss: 0.736646\n",
            "Train Epoch: 2 [21760/35339 (61%)]\tLoss: 0.603875\n",
            "Train Epoch: 2 [22400/35339 (63%)]\tLoss: 0.693971\n",
            "Train Epoch: 2 [23040/35339 (65%)]\tLoss: 0.901941\n",
            "Train Epoch: 2 [23680/35339 (67%)]\tLoss: 0.544535\n",
            "Train Epoch: 2 [24320/35339 (69%)]\tLoss: 0.730431\n",
            "Train Epoch: 2 [24960/35339 (71%)]\tLoss: 0.631173\n",
            "Train Epoch: 2 [25600/35339 (72%)]\tLoss: 0.639629\n",
            "Train Epoch: 2 [26240/35339 (74%)]\tLoss: 0.787200\n",
            "Train Epoch: 2 [26880/35339 (76%)]\tLoss: 0.689697\n",
            "Train Epoch: 2 [27520/35339 (78%)]\tLoss: 0.611388\n",
            "Train Epoch: 2 [28160/35339 (80%)]\tLoss: 0.709551\n",
            "Train Epoch: 2 [28800/35339 (81%)]\tLoss: 0.411271\n",
            "Train Epoch: 2 [29440/35339 (83%)]\tLoss: 0.536770\n",
            "Train Epoch: 2 [30080/35339 (85%)]\tLoss: 0.495294\n",
            "Train Epoch: 2 [30720/35339 (87%)]\tLoss: 0.516763\n",
            "Train Epoch: 2 [31360/35339 (89%)]\tLoss: 0.588621\n",
            "Train Epoch: 2 [32000/35339 (90%)]\tLoss: 0.650588\n",
            "Train Epoch: 2 [32640/35339 (92%)]\tLoss: 0.498436\n",
            "Train Epoch: 2 [33280/35339 (94%)]\tLoss: 0.596803\n",
            "Train Epoch: 2 [33920/35339 (96%)]\tLoss: 0.659694\n",
            "Train Epoch: 2 [34560/35339 (98%)]\tLoss: 0.781455\n",
            "Train Epoch: 2 [35200/35339 (99%)]\tLoss: 0.607862\n",
            "Training accuracy: 80.5399169921875\n",
            "\n",
            "Validation set: Average loss: 0.9836, Accuracy: 2959/3870 (76%)\n",
            "\n",
            "\n",
            "Saved model to model_2.pth.\n",
            "Train Epoch: 3 [0/35339 (0%)]\tLoss: 0.714763\n",
            "Train Epoch: 3 [640/35339 (2%)]\tLoss: 0.659994\n",
            "Train Epoch: 3 [1280/35339 (4%)]\tLoss: 0.505854\n",
            "Train Epoch: 3 [1920/35339 (5%)]\tLoss: 0.492543\n",
            "Train Epoch: 3 [2560/35339 (7%)]\tLoss: 0.576318\n",
            "Train Epoch: 3 [3200/35339 (9%)]\tLoss: 0.594658\n",
            "Train Epoch: 3 [3840/35339 (11%)]\tLoss: 0.695332\n",
            "Train Epoch: 3 [4480/35339 (13%)]\tLoss: 0.555727\n",
            "Train Epoch: 3 [5120/35339 (14%)]\tLoss: 0.426965\n",
            "Train Epoch: 3 [5760/35339 (16%)]\tLoss: 0.451110\n",
            "Train Epoch: 3 [6400/35339 (18%)]\tLoss: 0.521462\n",
            "Train Epoch: 3 [7040/35339 (20%)]\tLoss: 0.610404\n",
            "Train Epoch: 3 [7680/35339 (22%)]\tLoss: 0.464477\n",
            "Train Epoch: 3 [8320/35339 (24%)]\tLoss: 0.458054\n",
            "Train Epoch: 3 [8960/35339 (25%)]\tLoss: 0.426663\n",
            "Train Epoch: 3 [9600/35339 (27%)]\tLoss: 0.564075\n",
            "Train Epoch: 3 [10240/35339 (29%)]\tLoss: 0.504761\n",
            "Train Epoch: 3 [10880/35339 (31%)]\tLoss: 0.510082\n",
            "Train Epoch: 3 [11520/35339 (33%)]\tLoss: 0.281449\n",
            "Train Epoch: 3 [12160/35339 (34%)]\tLoss: 0.645287\n",
            "Train Epoch: 3 [12800/35339 (36%)]\tLoss: 0.387188\n",
            "Train Epoch: 3 [13440/35339 (38%)]\tLoss: 0.496028\n",
            "Train Epoch: 3 [14080/35339 (40%)]\tLoss: 0.450313\n",
            "Train Epoch: 3 [14720/35339 (42%)]\tLoss: 0.399143\n",
            "Train Epoch: 3 [15360/35339 (43%)]\tLoss: 0.509697\n",
            "Train Epoch: 3 [16000/35339 (45%)]\tLoss: 0.477545\n",
            "Train Epoch: 3 [16640/35339 (47%)]\tLoss: 0.454249\n",
            "Train Epoch: 3 [17280/35339 (49%)]\tLoss: 0.369735\n",
            "Train Epoch: 3 [17920/35339 (51%)]\tLoss: 0.545597\n",
            "Train Epoch: 3 [18560/35339 (52%)]\tLoss: 0.351373\n",
            "Train Epoch: 3 [19200/35339 (54%)]\tLoss: 0.480128\n",
            "Train Epoch: 3 [19840/35339 (56%)]\tLoss: 0.370692\n",
            "Train Epoch: 3 [20480/35339 (58%)]\tLoss: 0.416992\n",
            "Train Epoch: 3 [21120/35339 (60%)]\tLoss: 0.339950\n",
            "Train Epoch: 3 [21760/35339 (61%)]\tLoss: 0.330090\n",
            "Train Epoch: 3 [22400/35339 (63%)]\tLoss: 0.593533\n",
            "Train Epoch: 3 [23040/35339 (65%)]\tLoss: 0.588267\n",
            "Train Epoch: 3 [23680/35339 (67%)]\tLoss: 0.430872\n",
            "Train Epoch: 3 [24320/35339 (69%)]\tLoss: 0.397367\n",
            "Train Epoch: 3 [24960/35339 (71%)]\tLoss: 0.436577\n",
            "Train Epoch: 3 [25600/35339 (72%)]\tLoss: 0.372245\n",
            "Train Epoch: 3 [26240/35339 (74%)]\tLoss: 0.442115\n",
            "Train Epoch: 3 [26880/35339 (76%)]\tLoss: 0.424257\n",
            "Train Epoch: 3 [27520/35339 (78%)]\tLoss: 0.240693\n",
            "Train Epoch: 3 [28160/35339 (80%)]\tLoss: 0.395267\n",
            "Train Epoch: 3 [28800/35339 (81%)]\tLoss: 0.352870\n",
            "Train Epoch: 3 [29440/35339 (83%)]\tLoss: 0.349450\n",
            "Train Epoch: 3 [30080/35339 (85%)]\tLoss: 0.482098\n",
            "Train Epoch: 3 [30720/35339 (87%)]\tLoss: 0.437451\n",
            "Train Epoch: 3 [31360/35339 (89%)]\tLoss: 0.545796\n",
            "Train Epoch: 3 [32000/35339 (90%)]\tLoss: 0.337600\n",
            "Train Epoch: 3 [32640/35339 (92%)]\tLoss: 0.246574\n",
            "Train Epoch: 3 [33280/35339 (94%)]\tLoss: 0.360037\n",
            "Train Epoch: 3 [33920/35339 (96%)]\tLoss: 0.403452\n",
            "Train Epoch: 3 [34560/35339 (98%)]\tLoss: 0.465335\n",
            "Train Epoch: 3 [35200/35339 (99%)]\tLoss: 0.483071\n",
            "Training accuracy: 88.91310119628906\n",
            "\n",
            "Validation set: Average loss: 0.5880, Accuracy: 3369/3870 (87%)\n",
            "\n",
            "\n",
            "Saved model to model_3.pth.\n",
            "Train Epoch: 4 [0/35339 (0%)]\tLoss: 0.293402\n",
            "Train Epoch: 4 [640/35339 (2%)]\tLoss: 0.400279\n",
            "Train Epoch: 4 [1280/35339 (4%)]\tLoss: 0.351719\n",
            "Train Epoch: 4 [1920/35339 (5%)]\tLoss: 0.353961\n",
            "Train Epoch: 4 [2560/35339 (7%)]\tLoss: 0.222861\n",
            "Train Epoch: 4 [3200/35339 (9%)]\tLoss: 0.451582\n",
            "Train Epoch: 4 [3840/35339 (11%)]\tLoss: 0.371901\n",
            "Train Epoch: 4 [4480/35339 (13%)]\tLoss: 0.400570\n",
            "Train Epoch: 4 [5120/35339 (14%)]\tLoss: 0.232442\n",
            "Train Epoch: 4 [5760/35339 (16%)]\tLoss: 0.246885\n",
            "Train Epoch: 4 [6400/35339 (18%)]\tLoss: 0.392315\n",
            "Train Epoch: 4 [7040/35339 (20%)]\tLoss: 0.400201\n",
            "Train Epoch: 4 [7680/35339 (22%)]\tLoss: 0.167688\n",
            "Train Epoch: 4 [8320/35339 (24%)]\tLoss: 0.316749\n",
            "Train Epoch: 4 [8960/35339 (25%)]\tLoss: 0.266517\n",
            "Train Epoch: 4 [9600/35339 (27%)]\tLoss: 0.248115\n",
            "Train Epoch: 4 [10240/35339 (29%)]\tLoss: 0.175006\n",
            "Train Epoch: 4 [10880/35339 (31%)]\tLoss: 0.309560\n",
            "Train Epoch: 4 [11520/35339 (33%)]\tLoss: 0.371010\n",
            "Train Epoch: 4 [12160/35339 (34%)]\tLoss: 0.436092\n",
            "Train Epoch: 4 [12800/35339 (36%)]\tLoss: 0.360589\n",
            "Train Epoch: 4 [13440/35339 (38%)]\tLoss: 0.282884\n",
            "Train Epoch: 4 [14080/35339 (40%)]\tLoss: 0.216396\n",
            "Train Epoch: 4 [14720/35339 (42%)]\tLoss: 0.416720\n",
            "Train Epoch: 4 [15360/35339 (43%)]\tLoss: 0.351380\n",
            "Train Epoch: 4 [16000/35339 (45%)]\tLoss: 0.388213\n",
            "Train Epoch: 4 [16640/35339 (47%)]\tLoss: 0.206955\n",
            "Train Epoch: 4 [17280/35339 (49%)]\tLoss: 0.469060\n",
            "Train Epoch: 4 [17920/35339 (51%)]\tLoss: 0.348337\n",
            "Train Epoch: 4 [18560/35339 (52%)]\tLoss: 0.192698\n",
            "Train Epoch: 4 [19200/35339 (54%)]\tLoss: 0.290067\n",
            "Train Epoch: 4 [19840/35339 (56%)]\tLoss: 0.329515\n",
            "Train Epoch: 4 [20480/35339 (58%)]\tLoss: 0.268559\n",
            "Train Epoch: 4 [21120/35339 (60%)]\tLoss: 0.340949\n",
            "Train Epoch: 4 [21760/35339 (61%)]\tLoss: 0.191740\n",
            "Train Epoch: 4 [22400/35339 (63%)]\tLoss: 0.399116\n",
            "Train Epoch: 4 [23040/35339 (65%)]\tLoss: 0.245332\n",
            "Train Epoch: 4 [23680/35339 (67%)]\tLoss: 0.309139\n",
            "Train Epoch: 4 [24320/35339 (69%)]\tLoss: 0.245481\n",
            "Train Epoch: 4 [24960/35339 (71%)]\tLoss: 0.241171\n",
            "Train Epoch: 4 [25600/35339 (72%)]\tLoss: 0.302032\n",
            "Train Epoch: 4 [26240/35339 (74%)]\tLoss: 0.292477\n",
            "Train Epoch: 4 [26880/35339 (76%)]\tLoss: 0.249032\n",
            "Train Epoch: 4 [27520/35339 (78%)]\tLoss: 0.177057\n",
            "Train Epoch: 4 [28160/35339 (80%)]\tLoss: 0.182100\n",
            "Train Epoch: 4 [28800/35339 (81%)]\tLoss: 0.249186\n",
            "Train Epoch: 4 [29440/35339 (83%)]\tLoss: 0.089574\n",
            "Train Epoch: 4 [30080/35339 (85%)]\tLoss: 0.267835\n",
            "Train Epoch: 4 [30720/35339 (87%)]\tLoss: 0.347353\n",
            "Train Epoch: 4 [31360/35339 (89%)]\tLoss: 0.235749\n",
            "Train Epoch: 4 [32000/35339 (90%)]\tLoss: 0.196672\n",
            "Train Epoch: 4 [32640/35339 (92%)]\tLoss: 0.243807\n",
            "Train Epoch: 4 [33280/35339 (94%)]\tLoss: 0.162066\n",
            "Train Epoch: 4 [33920/35339 (96%)]\tLoss: 0.155544\n",
            "Train Epoch: 4 [34560/35339 (98%)]\tLoss: 0.361086\n",
            "Train Epoch: 4 [35200/35339 (99%)]\tLoss: 0.335994\n",
            "Training accuracy: 92.84925842285156\n",
            "\n",
            "Validation set: Average loss: 0.3810, Accuracy: 3507/3870 (91%)\n",
            "\n",
            "\n",
            "Saved model to model_4.pth.\n",
            "Train Epoch: 5 [0/35339 (0%)]\tLoss: 0.277276\n",
            "Train Epoch: 5 [640/35339 (2%)]\tLoss: 0.362245\n",
            "Train Epoch: 5 [1280/35339 (4%)]\tLoss: 0.199396\n",
            "Train Epoch: 5 [1920/35339 (5%)]\tLoss: 0.220171\n",
            "Train Epoch: 5 [2560/35339 (7%)]\tLoss: 0.195667\n",
            "Train Epoch: 5 [3200/35339 (9%)]\tLoss: 0.222618\n",
            "Train Epoch: 5 [3840/35339 (11%)]\tLoss: 0.102935\n",
            "Train Epoch: 5 [4480/35339 (13%)]\tLoss: 0.143928\n",
            "Train Epoch: 5 [5120/35339 (14%)]\tLoss: 0.178502\n",
            "Train Epoch: 5 [5760/35339 (16%)]\tLoss: 0.140285\n",
            "Train Epoch: 5 [6400/35339 (18%)]\tLoss: 0.344490\n",
            "Train Epoch: 5 [7040/35339 (20%)]\tLoss: 0.209193\n",
            "Train Epoch: 5 [7680/35339 (22%)]\tLoss: 0.224213\n",
            "Train Epoch: 5 [8320/35339 (24%)]\tLoss: 0.216624\n",
            "Train Epoch: 5 [8960/35339 (25%)]\tLoss: 0.276584\n",
            "Train Epoch: 5 [9600/35339 (27%)]\tLoss: 0.281527\n",
            "Train Epoch: 5 [10240/35339 (29%)]\tLoss: 0.270314\n",
            "Train Epoch: 5 [10880/35339 (31%)]\tLoss: 0.190200\n",
            "Train Epoch: 5 [11520/35339 (33%)]\tLoss: 0.155072\n",
            "Train Epoch: 5 [12160/35339 (34%)]\tLoss: 0.229727\n",
            "Train Epoch: 5 [12800/35339 (36%)]\tLoss: 0.189530\n",
            "Train Epoch: 5 [13440/35339 (38%)]\tLoss: 0.183273\n",
            "Train Epoch: 5 [14080/35339 (40%)]\tLoss: 0.321204\n",
            "Train Epoch: 5 [14720/35339 (42%)]\tLoss: 0.251704\n",
            "Train Epoch: 5 [15360/35339 (43%)]\tLoss: 0.196899\n",
            "Train Epoch: 5 [16000/35339 (45%)]\tLoss: 0.289215\n",
            "Train Epoch: 5 [16640/35339 (47%)]\tLoss: 0.217563\n",
            "Train Epoch: 5 [17280/35339 (49%)]\tLoss: 0.255595\n",
            "Train Epoch: 5 [17920/35339 (51%)]\tLoss: 0.181881\n",
            "Train Epoch: 5 [18560/35339 (52%)]\tLoss: 0.235211\n",
            "Train Epoch: 5 [19200/35339 (54%)]\tLoss: 0.259399\n",
            "Train Epoch: 5 [19840/35339 (56%)]\tLoss: 0.165638\n",
            "Train Epoch: 5 [20480/35339 (58%)]\tLoss: 0.153639\n",
            "Train Epoch: 5 [21120/35339 (60%)]\tLoss: 0.252001\n",
            "Train Epoch: 5 [21760/35339 (61%)]\tLoss: 0.162148\n",
            "Train Epoch: 5 [22400/35339 (63%)]\tLoss: 0.265945\n",
            "Train Epoch: 5 [23040/35339 (65%)]\tLoss: 0.189823\n",
            "Train Epoch: 5 [23680/35339 (67%)]\tLoss: 0.135426\n",
            "Train Epoch: 5 [24320/35339 (69%)]\tLoss: 0.095891\n",
            "Train Epoch: 5 [24960/35339 (71%)]\tLoss: 0.163592\n",
            "Train Epoch: 5 [25600/35339 (72%)]\tLoss: 0.185960\n",
            "Train Epoch: 5 [26240/35339 (74%)]\tLoss: 0.158051\n",
            "Train Epoch: 5 [26880/35339 (76%)]\tLoss: 0.118619\n",
            "Train Epoch: 5 [27520/35339 (78%)]\tLoss: 0.254423\n",
            "Train Epoch: 5 [28160/35339 (80%)]\tLoss: 0.278937\n",
            "Train Epoch: 5 [28800/35339 (81%)]\tLoss: 0.163281\n",
            "Train Epoch: 5 [29440/35339 (83%)]\tLoss: 0.175816\n",
            "Train Epoch: 5 [30080/35339 (85%)]\tLoss: 0.227537\n",
            "Train Epoch: 5 [30720/35339 (87%)]\tLoss: 0.246013\n",
            "Train Epoch: 5 [31360/35339 (89%)]\tLoss: 0.289749\n",
            "Train Epoch: 5 [32000/35339 (90%)]\tLoss: 0.154971\n",
            "Train Epoch: 5 [32640/35339 (92%)]\tLoss: 0.190887\n",
            "Train Epoch: 5 [33280/35339 (94%)]\tLoss: 0.199093\n",
            "Train Epoch: 5 [33920/35339 (96%)]\tLoss: 0.163921\n",
            "Train Epoch: 5 [34560/35339 (98%)]\tLoss: 0.168675\n",
            "Train Epoch: 5 [35200/35339 (99%)]\tLoss: 0.233451\n",
            "Training accuracy: 94.74800872802734\n",
            "\n",
            "Validation set: Average loss: 0.2821, Accuracy: 3604/3870 (93%)\n",
            "\n",
            "\n",
            "Saved model to model_5.pth.\n",
            "Train Epoch: 6 [0/35339 (0%)]\tLoss: 0.086474\n",
            "Train Epoch: 6 [640/35339 (2%)]\tLoss: 0.199624\n",
            "Train Epoch: 6 [1280/35339 (4%)]\tLoss: 0.183565\n",
            "Train Epoch: 6 [1920/35339 (5%)]\tLoss: 0.122764\n",
            "Train Epoch: 6 [2560/35339 (7%)]\tLoss: 0.190759\n",
            "Train Epoch: 6 [3200/35339 (9%)]\tLoss: 0.144261\n",
            "Train Epoch: 6 [3840/35339 (11%)]\tLoss: 0.206339\n",
            "Train Epoch: 6 [4480/35339 (13%)]\tLoss: 0.160552\n",
            "Train Epoch: 6 [5120/35339 (14%)]\tLoss: 0.196444\n",
            "Train Epoch: 6 [5760/35339 (16%)]\tLoss: 0.130731\n",
            "Train Epoch: 6 [6400/35339 (18%)]\tLoss: 0.209148\n",
            "Train Epoch: 6 [7040/35339 (20%)]\tLoss: 0.114989\n",
            "Train Epoch: 6 [7680/35339 (22%)]\tLoss: 0.132350\n",
            "Train Epoch: 6 [8320/35339 (24%)]\tLoss: 0.135759\n",
            "Train Epoch: 6 [8960/35339 (25%)]\tLoss: 0.275635\n",
            "Train Epoch: 6 [9600/35339 (27%)]\tLoss: 0.219198\n",
            "Train Epoch: 6 [10240/35339 (29%)]\tLoss: 0.197164\n",
            "Train Epoch: 6 [10880/35339 (31%)]\tLoss: 0.142470\n",
            "Train Epoch: 6 [11520/35339 (33%)]\tLoss: 0.183784\n",
            "Train Epoch: 6 [12160/35339 (34%)]\tLoss: 0.145258\n",
            "Train Epoch: 6 [12800/35339 (36%)]\tLoss: 0.246568\n",
            "Train Epoch: 6 [13440/35339 (38%)]\tLoss: 0.259006\n",
            "Train Epoch: 6 [14080/35339 (40%)]\tLoss: 0.126439\n",
            "Train Epoch: 6 [14720/35339 (42%)]\tLoss: 0.290643\n",
            "Train Epoch: 6 [15360/35339 (43%)]\tLoss: 0.108173\n",
            "Train Epoch: 6 [16000/35339 (45%)]\tLoss: 0.262512\n",
            "Train Epoch: 6 [16640/35339 (47%)]\tLoss: 0.084370\n",
            "Train Epoch: 6 [17280/35339 (49%)]\tLoss: 0.241101\n",
            "Train Epoch: 6 [17920/35339 (51%)]\tLoss: 0.128771\n",
            "Train Epoch: 6 [18560/35339 (52%)]\tLoss: 0.236861\n",
            "Train Epoch: 6 [19200/35339 (54%)]\tLoss: 0.092346\n",
            "Train Epoch: 6 [19840/35339 (56%)]\tLoss: 0.184303\n",
            "Train Epoch: 6 [20480/35339 (58%)]\tLoss: 0.309638\n",
            "Train Epoch: 6 [21120/35339 (60%)]\tLoss: 0.123019\n",
            "Train Epoch: 6 [21760/35339 (61%)]\tLoss: 0.127622\n",
            "Train Epoch: 6 [22400/35339 (63%)]\tLoss: 0.152937\n",
            "Train Epoch: 6 [23040/35339 (65%)]\tLoss: 0.100589\n",
            "Train Epoch: 6 [23680/35339 (67%)]\tLoss: 0.074956\n",
            "Train Epoch: 6 [24320/35339 (69%)]\tLoss: 0.109187\n",
            "Train Epoch: 6 [24960/35339 (71%)]\tLoss: 0.130185\n",
            "Train Epoch: 6 [25600/35339 (72%)]\tLoss: 0.140813\n",
            "Train Epoch: 6 [26240/35339 (74%)]\tLoss: 0.085898\n",
            "Train Epoch: 6 [26880/35339 (76%)]\tLoss: 0.231302\n",
            "Train Epoch: 6 [27520/35339 (78%)]\tLoss: 0.218890\n",
            "Train Epoch: 6 [28160/35339 (80%)]\tLoss: 0.127634\n",
            "Train Epoch: 6 [28800/35339 (81%)]\tLoss: 0.116529\n",
            "Train Epoch: 6 [29440/35339 (83%)]\tLoss: 0.179661\n",
            "Train Epoch: 6 [30080/35339 (85%)]\tLoss: 0.131377\n",
            "Train Epoch: 6 [30720/35339 (87%)]\tLoss: 0.150551\n",
            "Train Epoch: 6 [31360/35339 (89%)]\tLoss: 0.176734\n",
            "Train Epoch: 6 [32000/35339 (90%)]\tLoss: 0.126594\n",
            "Train Epoch: 6 [32640/35339 (92%)]\tLoss: 0.144614\n",
            "Train Epoch: 6 [33280/35339 (94%)]\tLoss: 0.098230\n",
            "Train Epoch: 6 [33920/35339 (96%)]\tLoss: 0.092243\n",
            "Train Epoch: 6 [34560/35339 (98%)]\tLoss: 0.202069\n",
            "Train Epoch: 6 [35200/35339 (99%)]\tLoss: 0.055368\n",
            "Training accuracy: 96.0298843383789\n",
            "\n",
            "Validation set: Average loss: 0.2857, Accuracy: 3579/3870 (92%)\n",
            "\n",
            "\n",
            "Saved model to model_6.pth.\n",
            "Train Epoch: 7 [0/35339 (0%)]\tLoss: 0.102210\n",
            "Train Epoch: 7 [640/35339 (2%)]\tLoss: 0.175318\n",
            "Train Epoch: 7 [1280/35339 (4%)]\tLoss: 0.127106\n",
            "Train Epoch: 7 [1920/35339 (5%)]\tLoss: 0.206944\n",
            "Train Epoch: 7 [2560/35339 (7%)]\tLoss: 0.117587\n",
            "Train Epoch: 7 [3200/35339 (9%)]\tLoss: 0.325861\n",
            "Train Epoch: 7 [3840/35339 (11%)]\tLoss: 0.108212\n",
            "Train Epoch: 7 [4480/35339 (13%)]\tLoss: 0.154563\n",
            "Train Epoch: 7 [5120/35339 (14%)]\tLoss: 0.188791\n",
            "Train Epoch: 7 [5760/35339 (16%)]\tLoss: 0.115214\n",
            "Train Epoch: 7 [6400/35339 (18%)]\tLoss: 0.120264\n",
            "Train Epoch: 7 [7040/35339 (20%)]\tLoss: 0.156717\n",
            "Train Epoch: 7 [7680/35339 (22%)]\tLoss: 0.078925\n",
            "Train Epoch: 7 [8320/35339 (24%)]\tLoss: 0.088486\n",
            "Train Epoch: 7 [8960/35339 (25%)]\tLoss: 0.130972\n",
            "Train Epoch: 7 [9600/35339 (27%)]\tLoss: 0.204946\n",
            "Train Epoch: 7 [10240/35339 (29%)]\tLoss: 0.141307\n",
            "Train Epoch: 7 [10880/35339 (31%)]\tLoss: 0.109141\n",
            "Train Epoch: 7 [11520/35339 (33%)]\tLoss: 0.140767\n",
            "Train Epoch: 7 [12160/35339 (34%)]\tLoss: 0.142936\n",
            "Train Epoch: 7 [12800/35339 (36%)]\tLoss: 0.109613\n",
            "Train Epoch: 7 [13440/35339 (38%)]\tLoss: 0.116327\n",
            "Train Epoch: 7 [14080/35339 (40%)]\tLoss: 0.102950\n",
            "Train Epoch: 7 [14720/35339 (42%)]\tLoss: 0.072800\n",
            "Train Epoch: 7 [15360/35339 (43%)]\tLoss: 0.058361\n",
            "Train Epoch: 7 [16000/35339 (45%)]\tLoss: 0.153772\n",
            "Train Epoch: 7 [16640/35339 (47%)]\tLoss: 0.081797\n",
            "Train Epoch: 7 [17280/35339 (49%)]\tLoss: 0.113121\n",
            "Train Epoch: 7 [17920/35339 (51%)]\tLoss: 0.201302\n",
            "Train Epoch: 7 [18560/35339 (52%)]\tLoss: 0.162777\n",
            "Train Epoch: 7 [19200/35339 (54%)]\tLoss: 0.130466\n",
            "Train Epoch: 7 [19840/35339 (56%)]\tLoss: 0.072822\n",
            "Train Epoch: 7 [20480/35339 (58%)]\tLoss: 0.201391\n",
            "Train Epoch: 7 [21120/35339 (60%)]\tLoss: 0.093038\n",
            "Train Epoch: 7 [21760/35339 (61%)]\tLoss: 0.090670\n",
            "Train Epoch: 7 [22400/35339 (63%)]\tLoss: 0.098139\n",
            "Train Epoch: 7 [23040/35339 (65%)]\tLoss: 0.091015\n",
            "Train Epoch: 7 [23680/35339 (67%)]\tLoss: 0.228463\n",
            "Train Epoch: 7 [24320/35339 (69%)]\tLoss: 0.067313\n",
            "Train Epoch: 7 [24960/35339 (71%)]\tLoss: 0.141377\n",
            "Train Epoch: 7 [25600/35339 (72%)]\tLoss: 0.141877\n",
            "Train Epoch: 7 [26240/35339 (74%)]\tLoss: 0.078896\n",
            "Train Epoch: 7 [26880/35339 (76%)]\tLoss: 0.050553\n",
            "Train Epoch: 7 [27520/35339 (78%)]\tLoss: 0.109760\n",
            "Train Epoch: 7 [28160/35339 (80%)]\tLoss: 0.136514\n",
            "Train Epoch: 7 [28800/35339 (81%)]\tLoss: 0.102295\n",
            "Train Epoch: 7 [29440/35339 (83%)]\tLoss: 0.107848\n",
            "Train Epoch: 7 [30080/35339 (85%)]\tLoss: 0.200504\n",
            "Train Epoch: 7 [30720/35339 (87%)]\tLoss: 0.106376\n",
            "Train Epoch: 7 [31360/35339 (89%)]\tLoss: 0.035637\n",
            "Train Epoch: 7 [32000/35339 (90%)]\tLoss: 0.132957\n",
            "Train Epoch: 7 [32640/35339 (92%)]\tLoss: 0.171676\n",
            "Train Epoch: 7 [33280/35339 (94%)]\tLoss: 0.055640\n",
            "Train Epoch: 7 [33920/35339 (96%)]\tLoss: 0.135385\n",
            "Train Epoch: 7 [34560/35339 (98%)]\tLoss: 0.076446\n",
            "Train Epoch: 7 [35200/35339 (99%)]\tLoss: 0.107161\n",
            "Training accuracy: 96.58168029785156\n",
            "\n",
            "Validation set: Average loss: 0.2235, Accuracy: 3643/3870 (94%)\n",
            "\n",
            "\n",
            "Saved model to model_7.pth.\n",
            "Train Epoch: 8 [0/35339 (0%)]\tLoss: 0.140295\n",
            "Train Epoch: 8 [640/35339 (2%)]\tLoss: 0.087532\n",
            "Train Epoch: 8 [1280/35339 (4%)]\tLoss: 0.106723\n",
            "Train Epoch: 8 [1920/35339 (5%)]\tLoss: 0.141778\n",
            "Train Epoch: 8 [2560/35339 (7%)]\tLoss: 0.039046\n",
            "Train Epoch: 8 [3200/35339 (9%)]\tLoss: 0.125018\n",
            "Train Epoch: 8 [3840/35339 (11%)]\tLoss: 0.102367\n",
            "Train Epoch: 8 [4480/35339 (13%)]\tLoss: 0.111362\n",
            "Train Epoch: 8 [5120/35339 (14%)]\tLoss: 0.123815\n",
            "Train Epoch: 8 [5760/35339 (16%)]\tLoss: 0.083134\n",
            "Train Epoch: 8 [6400/35339 (18%)]\tLoss: 0.114469\n",
            "Train Epoch: 8 [7040/35339 (20%)]\tLoss: 0.126051\n",
            "Train Epoch: 8 [7680/35339 (22%)]\tLoss: 0.114121\n",
            "Train Epoch: 8 [8320/35339 (24%)]\tLoss: 0.085355\n",
            "Train Epoch: 8 [8960/35339 (25%)]\tLoss: 0.114874\n",
            "Train Epoch: 8 [9600/35339 (27%)]\tLoss: 0.063430\n",
            "Train Epoch: 8 [10240/35339 (29%)]\tLoss: 0.113400\n",
            "Train Epoch: 8 [10880/35339 (31%)]\tLoss: 0.063027\n",
            "Train Epoch: 8 [11520/35339 (33%)]\tLoss: 0.129747\n",
            "Train Epoch: 8 [12160/35339 (34%)]\tLoss: 0.034013\n",
            "Train Epoch: 8 [12800/35339 (36%)]\tLoss: 0.068178\n",
            "Train Epoch: 8 [13440/35339 (38%)]\tLoss: 0.273246\n",
            "Train Epoch: 8 [14080/35339 (40%)]\tLoss: 0.114177\n",
            "Train Epoch: 8 [14720/35339 (42%)]\tLoss: 0.044278\n",
            "Train Epoch: 8 [15360/35339 (43%)]\tLoss: 0.076904\n",
            "Train Epoch: 8 [16000/35339 (45%)]\tLoss: 0.202567\n",
            "Train Epoch: 8 [16640/35339 (47%)]\tLoss: 0.099867\n",
            "Train Epoch: 8 [17280/35339 (49%)]\tLoss: 0.021662\n",
            "Train Epoch: 8 [17920/35339 (51%)]\tLoss: 0.095205\n",
            "Train Epoch: 8 [18560/35339 (52%)]\tLoss: 0.221608\n",
            "Train Epoch: 8 [19200/35339 (54%)]\tLoss: 0.107481\n",
            "Train Epoch: 8 [19840/35339 (56%)]\tLoss: 0.120507\n",
            "Train Epoch: 8 [20480/35339 (58%)]\tLoss: 0.048419\n",
            "Train Epoch: 8 [21120/35339 (60%)]\tLoss: 0.057966\n",
            "Train Epoch: 8 [21760/35339 (61%)]\tLoss: 0.061997\n",
            "Train Epoch: 8 [22400/35339 (63%)]\tLoss: 0.109363\n",
            "Train Epoch: 8 [23040/35339 (65%)]\tLoss: 0.158775\n",
            "Train Epoch: 8 [23680/35339 (67%)]\tLoss: 0.087897\n",
            "Train Epoch: 8 [24320/35339 (69%)]\tLoss: 0.120116\n",
            "Train Epoch: 8 [24960/35339 (71%)]\tLoss: 0.109604\n",
            "Train Epoch: 8 [25600/35339 (72%)]\tLoss: 0.137015\n",
            "Train Epoch: 8 [26240/35339 (74%)]\tLoss: 0.101897\n",
            "Train Epoch: 8 [26880/35339 (76%)]\tLoss: 0.042407\n",
            "Train Epoch: 8 [27520/35339 (78%)]\tLoss: 0.082490\n",
            "Train Epoch: 8 [28160/35339 (80%)]\tLoss: 0.088327\n",
            "Train Epoch: 8 [28800/35339 (81%)]\tLoss: 0.059296\n",
            "Train Epoch: 8 [29440/35339 (83%)]\tLoss: 0.065443\n",
            "Train Epoch: 8 [30080/35339 (85%)]\tLoss: 0.115153\n",
            "Train Epoch: 8 [30720/35339 (87%)]\tLoss: 0.152529\n",
            "Train Epoch: 8 [31360/35339 (89%)]\tLoss: 0.110867\n",
            "Train Epoch: 8 [32000/35339 (90%)]\tLoss: 0.072449\n",
            "Train Epoch: 8 [32640/35339 (92%)]\tLoss: 0.112605\n",
            "Train Epoch: 8 [33280/35339 (94%)]\tLoss: 0.156950\n",
            "Train Epoch: 8 [33920/35339 (96%)]\tLoss: 0.133277\n",
            "Train Epoch: 8 [34560/35339 (98%)]\tLoss: 0.048687\n",
            "Train Epoch: 8 [35200/35339 (99%)]\tLoss: 0.107433\n",
            "Training accuracy: 97.2013931274414\n",
            "\n",
            "Validation set: Average loss: 0.2443, Accuracy: 3596/3870 (93%)\n",
            "\n",
            "\n",
            "Saved model to model_8.pth.\n",
            "Train Epoch: 9 [0/35339 (0%)]\tLoss: 0.085240\n",
            "Train Epoch: 9 [640/35339 (2%)]\tLoss: 0.245999\n",
            "Train Epoch: 9 [1280/35339 (4%)]\tLoss: 0.192962\n",
            "Train Epoch: 9 [1920/35339 (5%)]\tLoss: 0.064814\n",
            "Train Epoch: 9 [2560/35339 (7%)]\tLoss: 0.092463\n",
            "Train Epoch: 9 [3200/35339 (9%)]\tLoss: 0.036212\n",
            "Train Epoch: 9 [3840/35339 (11%)]\tLoss: 0.060623\n",
            "Train Epoch: 9 [4480/35339 (13%)]\tLoss: 0.068619\n",
            "Train Epoch: 9 [5120/35339 (14%)]\tLoss: 0.037136\n",
            "Train Epoch: 9 [5760/35339 (16%)]\tLoss: 0.192617\n",
            "Train Epoch: 9 [6400/35339 (18%)]\tLoss: 0.047914\n",
            "Train Epoch: 9 [7040/35339 (20%)]\tLoss: 0.096368\n",
            "Train Epoch: 9 [7680/35339 (22%)]\tLoss: 0.101009\n",
            "Train Epoch: 9 [8320/35339 (24%)]\tLoss: 0.122172\n",
            "Train Epoch: 9 [8960/35339 (25%)]\tLoss: 0.278894\n",
            "Train Epoch: 9 [9600/35339 (27%)]\tLoss: 0.198127\n",
            "Train Epoch: 9 [10240/35339 (29%)]\tLoss: 0.203630\n",
            "Train Epoch: 9 [10880/35339 (31%)]\tLoss: 0.073293\n",
            "Train Epoch: 9 [11520/35339 (33%)]\tLoss: 0.143475\n",
            "Train Epoch: 9 [12160/35339 (34%)]\tLoss: 0.077770\n",
            "Train Epoch: 9 [12800/35339 (36%)]\tLoss: 0.099813\n",
            "Train Epoch: 9 [13440/35339 (38%)]\tLoss: 0.115491\n",
            "Train Epoch: 9 [14080/35339 (40%)]\tLoss: 0.058109\n",
            "Train Epoch: 9 [14720/35339 (42%)]\tLoss: 0.063044\n",
            "Train Epoch: 9 [15360/35339 (43%)]\tLoss: 0.089332\n",
            "Train Epoch: 9 [16000/35339 (45%)]\tLoss: 0.094032\n",
            "Train Epoch: 9 [16640/35339 (47%)]\tLoss: 0.161913\n",
            "Train Epoch: 9 [17280/35339 (49%)]\tLoss: 0.046156\n",
            "Train Epoch: 9 [17920/35339 (51%)]\tLoss: 0.059739\n",
            "Train Epoch: 9 [18560/35339 (52%)]\tLoss: 0.073805\n",
            "Train Epoch: 9 [19200/35339 (54%)]\tLoss: 0.095740\n",
            "Train Epoch: 9 [19840/35339 (56%)]\tLoss: 0.135595\n",
            "Train Epoch: 9 [20480/35339 (58%)]\tLoss: 0.107404\n",
            "Train Epoch: 9 [21120/35339 (60%)]\tLoss: 0.061974\n",
            "Train Epoch: 9 [21760/35339 (61%)]\tLoss: 0.095803\n",
            "Train Epoch: 9 [22400/35339 (63%)]\tLoss: 0.113979\n",
            "Train Epoch: 9 [23040/35339 (65%)]\tLoss: 0.062078\n",
            "Train Epoch: 9 [23680/35339 (67%)]\tLoss: 0.040747\n",
            "Train Epoch: 9 [24320/35339 (69%)]\tLoss: 0.043922\n",
            "Train Epoch: 9 [24960/35339 (71%)]\tLoss: 0.075281\n",
            "Train Epoch: 9 [25600/35339 (72%)]\tLoss: 0.107998\n",
            "Train Epoch: 9 [26240/35339 (74%)]\tLoss: 0.104696\n",
            "Train Epoch: 9 [26880/35339 (76%)]\tLoss: 0.062275\n",
            "Train Epoch: 9 [27520/35339 (78%)]\tLoss: 0.142206\n",
            "Train Epoch: 9 [28160/35339 (80%)]\tLoss: 0.037228\n",
            "Train Epoch: 9 [28800/35339 (81%)]\tLoss: 0.177540\n",
            "Train Epoch: 9 [29440/35339 (83%)]\tLoss: 0.084511\n",
            "Train Epoch: 9 [30080/35339 (85%)]\tLoss: 0.069299\n",
            "Train Epoch: 9 [30720/35339 (87%)]\tLoss: 0.108685\n",
            "Train Epoch: 9 [31360/35339 (89%)]\tLoss: 0.143676\n",
            "Train Epoch: 9 [32000/35339 (90%)]\tLoss: 0.109381\n",
            "Train Epoch: 9 [32640/35339 (92%)]\tLoss: 0.061029\n",
            "Train Epoch: 9 [33280/35339 (94%)]\tLoss: 0.105581\n",
            "Train Epoch: 9 [33920/35339 (96%)]\tLoss: 0.060200\n",
            "Train Epoch: 9 [34560/35339 (98%)]\tLoss: 0.037395\n",
            "Train Epoch: 9 [35200/35339 (99%)]\tLoss: 0.098224\n",
            "Training accuracy: 97.40796661376953\n",
            "\n",
            "Validation set: Average loss: 0.2172, Accuracy: 3639/3870 (94%)\n",
            "\n",
            "\n",
            "Saved model to model_9.pth.\n",
            "Train Epoch: 10 [0/35339 (0%)]\tLoss: 0.095015\n",
            "Train Epoch: 10 [640/35339 (2%)]\tLoss: 0.084088\n",
            "Train Epoch: 10 [1280/35339 (4%)]\tLoss: 0.078011\n",
            "Train Epoch: 10 [1920/35339 (5%)]\tLoss: 0.100971\n",
            "Train Epoch: 10 [2560/35339 (7%)]\tLoss: 0.119079\n",
            "Train Epoch: 10 [3200/35339 (9%)]\tLoss: 0.061749\n",
            "Train Epoch: 10 [3840/35339 (11%)]\tLoss: 0.046390\n",
            "Train Epoch: 10 [4480/35339 (13%)]\tLoss: 0.104293\n",
            "Train Epoch: 10 [5120/35339 (14%)]\tLoss: 0.161740\n",
            "Train Epoch: 10 [5760/35339 (16%)]\tLoss: 0.047995\n",
            "Train Epoch: 10 [6400/35339 (18%)]\tLoss: 0.031718\n",
            "Train Epoch: 10 [7040/35339 (20%)]\tLoss: 0.089986\n",
            "Train Epoch: 10 [7680/35339 (22%)]\tLoss: 0.043680\n",
            "Train Epoch: 10 [8320/35339 (24%)]\tLoss: 0.040127\n",
            "Train Epoch: 10 [8960/35339 (25%)]\tLoss: 0.116708\n",
            "Train Epoch: 10 [9600/35339 (27%)]\tLoss: 0.046880\n",
            "Train Epoch: 10 [10240/35339 (29%)]\tLoss: 0.091357\n",
            "Train Epoch: 10 [10880/35339 (31%)]\tLoss: 0.021068\n",
            "Train Epoch: 10 [11520/35339 (33%)]\tLoss: 0.129565\n",
            "Train Epoch: 10 [12160/35339 (34%)]\tLoss: 0.095903\n",
            "Train Epoch: 10 [12800/35339 (36%)]\tLoss: 0.141784\n",
            "Train Epoch: 10 [13440/35339 (38%)]\tLoss: 0.125921\n",
            "Train Epoch: 10 [14080/35339 (40%)]\tLoss: 0.067832\n",
            "Train Epoch: 10 [14720/35339 (42%)]\tLoss: 0.159805\n",
            "Train Epoch: 10 [15360/35339 (43%)]\tLoss: 0.048654\n",
            "Train Epoch: 10 [16000/35339 (45%)]\tLoss: 0.077672\n",
            "Train Epoch: 10 [16640/35339 (47%)]\tLoss: 0.095914\n",
            "Train Epoch: 10 [17280/35339 (49%)]\tLoss: 0.080036\n",
            "Train Epoch: 10 [17920/35339 (51%)]\tLoss: 0.041057\n",
            "Train Epoch: 10 [18560/35339 (52%)]\tLoss: 0.095736\n",
            "Train Epoch: 10 [19200/35339 (54%)]\tLoss: 0.081182\n",
            "Train Epoch: 10 [19840/35339 (56%)]\tLoss: 0.073745\n",
            "Train Epoch: 10 [20480/35339 (58%)]\tLoss: 0.070702\n",
            "Train Epoch: 10 [21120/35339 (60%)]\tLoss: 0.073649\n",
            "Train Epoch: 10 [21760/35339 (61%)]\tLoss: 0.171594\n",
            "Train Epoch: 10 [22400/35339 (63%)]\tLoss: 0.073655\n",
            "Train Epoch: 10 [23040/35339 (65%)]\tLoss: 0.100064\n",
            "Train Epoch: 10 [23680/35339 (67%)]\tLoss: 0.173125\n",
            "Train Epoch: 10 [24320/35339 (69%)]\tLoss: 0.027827\n",
            "Train Epoch: 10 [24960/35339 (71%)]\tLoss: 0.096018\n",
            "Train Epoch: 10 [25600/35339 (72%)]\tLoss: 0.039670\n",
            "Train Epoch: 10 [26240/35339 (74%)]\tLoss: 0.065910\n",
            "Train Epoch: 10 [26880/35339 (76%)]\tLoss: 0.105324\n",
            "Train Epoch: 10 [27520/35339 (78%)]\tLoss: 0.089892\n",
            "Train Epoch: 10 [28160/35339 (80%)]\tLoss: 0.056708\n",
            "Train Epoch: 10 [28800/35339 (81%)]\tLoss: 0.061909\n",
            "Train Epoch: 10 [29440/35339 (83%)]\tLoss: 0.167891\n",
            "Train Epoch: 10 [30080/35339 (85%)]\tLoss: 0.118712\n",
            "Train Epoch: 10 [30720/35339 (87%)]\tLoss: 0.016338\n",
            "Train Epoch: 10 [31360/35339 (89%)]\tLoss: 0.127947\n",
            "Train Epoch: 10 [32000/35339 (90%)]\tLoss: 0.056643\n",
            "Train Epoch: 10 [32640/35339 (92%)]\tLoss: 0.072859\n",
            "Train Epoch: 10 [33280/35339 (94%)]\tLoss: 0.059348\n",
            "Train Epoch: 10 [33920/35339 (96%)]\tLoss: 0.107323\n",
            "Train Epoch: 10 [34560/35339 (98%)]\tLoss: 0.067367\n",
            "Train Epoch: 10 [35200/35339 (99%)]\tLoss: 0.141651\n",
            "Training accuracy: 97.6881103515625\n",
            "\n",
            "Validation set: Average loss: 0.2078, Accuracy: 3665/3870 (95%)\n",
            "\n",
            "\n",
            "Saved model to model_10.pth.\n",
            "Train Epoch: 11 [0/35339 (0%)]\tLoss: 0.067623\n",
            "Train Epoch: 11 [640/35339 (2%)]\tLoss: 0.081611\n",
            "Train Epoch: 11 [1280/35339 (4%)]\tLoss: 0.131044\n",
            "Train Epoch: 11 [1920/35339 (5%)]\tLoss: 0.034462\n",
            "Train Epoch: 11 [2560/35339 (7%)]\tLoss: 0.080771\n",
            "Train Epoch: 11 [3200/35339 (9%)]\tLoss: 0.117787\n",
            "Train Epoch: 11 [3840/35339 (11%)]\tLoss: 0.071082\n",
            "Train Epoch: 11 [4480/35339 (13%)]\tLoss: 0.126456\n",
            "Train Epoch: 11 [5120/35339 (14%)]\tLoss: 0.046023\n",
            "Train Epoch: 11 [5760/35339 (16%)]\tLoss: 0.043185\n",
            "Train Epoch: 11 [6400/35339 (18%)]\tLoss: 0.044587\n",
            "Train Epoch: 11 [7040/35339 (20%)]\tLoss: 0.077901\n",
            "Train Epoch: 11 [7680/35339 (22%)]\tLoss: 0.067883\n",
            "Train Epoch: 11 [8320/35339 (24%)]\tLoss: 0.105491\n",
            "Train Epoch: 11 [8960/35339 (25%)]\tLoss: 0.038967\n",
            "Train Epoch: 11 [9600/35339 (27%)]\tLoss: 0.117829\n",
            "Train Epoch: 11 [10240/35339 (29%)]\tLoss: 0.078932\n",
            "Train Epoch: 11 [10880/35339 (31%)]\tLoss: 0.042487\n",
            "Train Epoch: 11 [11520/35339 (33%)]\tLoss: 0.049399\n",
            "Train Epoch: 11 [12160/35339 (34%)]\tLoss: 0.084620\n",
            "Train Epoch: 11 [12800/35339 (36%)]\tLoss: 0.101751\n",
            "Train Epoch: 11 [13440/35339 (38%)]\tLoss: 0.159906\n",
            "Train Epoch: 11 [14080/35339 (40%)]\tLoss: 0.107854\n",
            "Train Epoch: 11 [14720/35339 (42%)]\tLoss: 0.037142\n",
            "Train Epoch: 11 [15360/35339 (43%)]\tLoss: 0.031223\n",
            "Train Epoch: 11 [16000/35339 (45%)]\tLoss: 0.200271\n",
            "Train Epoch: 11 [16640/35339 (47%)]\tLoss: 0.111502\n",
            "Train Epoch: 11 [17280/35339 (49%)]\tLoss: 0.062681\n",
            "Train Epoch: 11 [17920/35339 (51%)]\tLoss: 0.170651\n",
            "Train Epoch: 11 [18560/35339 (52%)]\tLoss: 0.022073\n",
            "Train Epoch: 11 [19200/35339 (54%)]\tLoss: 0.108882\n",
            "Train Epoch: 11 [19840/35339 (56%)]\tLoss: 0.072533\n",
            "Train Epoch: 11 [20480/35339 (58%)]\tLoss: 0.069440\n",
            "Train Epoch: 11 [21120/35339 (60%)]\tLoss: 0.049496\n",
            "Train Epoch: 11 [21760/35339 (61%)]\tLoss: 0.039972\n",
            "Train Epoch: 11 [22400/35339 (63%)]\tLoss: 0.067868\n",
            "Train Epoch: 11 [23040/35339 (65%)]\tLoss: 0.097144\n",
            "Train Epoch: 11 [23680/35339 (67%)]\tLoss: 0.076809\n",
            "Train Epoch: 11 [24320/35339 (69%)]\tLoss: 0.130615\n",
            "Train Epoch: 11 [24960/35339 (71%)]\tLoss: 0.112755\n",
            "Train Epoch: 11 [25600/35339 (72%)]\tLoss: 0.086057\n",
            "Train Epoch: 11 [26240/35339 (74%)]\tLoss: 0.048259\n",
            "Train Epoch: 11 [26880/35339 (76%)]\tLoss: 0.068660\n",
            "Train Epoch: 11 [27520/35339 (78%)]\tLoss: 0.091748\n",
            "Train Epoch: 11 [28160/35339 (80%)]\tLoss: 0.059622\n",
            "Train Epoch: 11 [28800/35339 (81%)]\tLoss: 0.083051\n",
            "Train Epoch: 11 [29440/35339 (83%)]\tLoss: 0.063097\n",
            "Train Epoch: 11 [30080/35339 (85%)]\tLoss: 0.076401\n",
            "Train Epoch: 11 [30720/35339 (87%)]\tLoss: 0.065292\n",
            "Train Epoch: 11 [31360/35339 (89%)]\tLoss: 0.057025\n",
            "Train Epoch: 11 [32000/35339 (90%)]\tLoss: 0.071511\n",
            "Train Epoch: 11 [32640/35339 (92%)]\tLoss: 0.062050\n",
            "Train Epoch: 11 [33280/35339 (94%)]\tLoss: 0.053964\n",
            "Train Epoch: 11 [33920/35339 (96%)]\tLoss: 0.053187\n",
            "Train Epoch: 11 [34560/35339 (98%)]\tLoss: 0.146115\n",
            "Train Epoch: 11 [35200/35339 (99%)]\tLoss: 0.053235\n",
            "Training accuracy: 98.03899383544922\n",
            "\n",
            "Validation set: Average loss: 0.2405, Accuracy: 3651/3870 (94%)\n",
            "\n",
            "\n",
            "Saved model to model_11.pth.\n",
            "Train Epoch: 12 [0/35339 (0%)]\tLoss: 0.093991\n",
            "Train Epoch: 12 [640/35339 (2%)]\tLoss: 0.114045\n",
            "Train Epoch: 12 [1280/35339 (4%)]\tLoss: 0.023596\n",
            "Train Epoch: 12 [1920/35339 (5%)]\tLoss: 0.123811\n",
            "Train Epoch: 12 [2560/35339 (7%)]\tLoss: 0.089320\n",
            "Train Epoch: 12 [3200/35339 (9%)]\tLoss: 0.058456\n",
            "Train Epoch: 12 [3840/35339 (11%)]\tLoss: 0.071914\n",
            "Train Epoch: 12 [4480/35339 (13%)]\tLoss: 0.070711\n",
            "Train Epoch: 12 [5120/35339 (14%)]\tLoss: 0.058902\n",
            "Train Epoch: 12 [5760/35339 (16%)]\tLoss: 0.073704\n",
            "Train Epoch: 12 [6400/35339 (18%)]\tLoss: 0.078600\n",
            "Train Epoch: 12 [7040/35339 (20%)]\tLoss: 0.025647\n",
            "Train Epoch: 12 [7680/35339 (22%)]\tLoss: 0.045417\n",
            "Train Epoch: 12 [8320/35339 (24%)]\tLoss: 0.034333\n",
            "Train Epoch: 12 [8960/35339 (25%)]\tLoss: 0.085502\n",
            "Train Epoch: 12 [9600/35339 (27%)]\tLoss: 0.077110\n",
            "Train Epoch: 12 [10240/35339 (29%)]\tLoss: 0.129438\n",
            "Train Epoch: 12 [10880/35339 (31%)]\tLoss: 0.060525\n",
            "Train Epoch: 12 [11520/35339 (33%)]\tLoss: 0.028435\n",
            "Train Epoch: 12 [12160/35339 (34%)]\tLoss: 0.068305\n",
            "Train Epoch: 12 [12800/35339 (36%)]\tLoss: 0.043497\n",
            "Train Epoch: 12 [13440/35339 (38%)]\tLoss: 0.077283\n",
            "Train Epoch: 12 [14080/35339 (40%)]\tLoss: 0.123496\n",
            "Train Epoch: 12 [14720/35339 (42%)]\tLoss: 0.104526\n",
            "Train Epoch: 12 [15360/35339 (43%)]\tLoss: 0.132886\n",
            "Train Epoch: 12 [16000/35339 (45%)]\tLoss: 0.042677\n",
            "Train Epoch: 12 [16640/35339 (47%)]\tLoss: 0.037622\n",
            "Train Epoch: 12 [17280/35339 (49%)]\tLoss: 0.041591\n",
            "Train Epoch: 12 [17920/35339 (51%)]\tLoss: 0.056469\n",
            "Train Epoch: 12 [18560/35339 (52%)]\tLoss: 0.049714\n",
            "Train Epoch: 12 [19200/35339 (54%)]\tLoss: 0.077342\n",
            "Train Epoch: 12 [19840/35339 (56%)]\tLoss: 0.068977\n",
            "Train Epoch: 12 [20480/35339 (58%)]\tLoss: 0.068484\n",
            "Train Epoch: 12 [21120/35339 (60%)]\tLoss: 0.084185\n",
            "Train Epoch: 12 [21760/35339 (61%)]\tLoss: 0.142570\n",
            "Train Epoch: 12 [22400/35339 (63%)]\tLoss: 0.053627\n",
            "Train Epoch: 12 [23040/35339 (65%)]\tLoss: 0.106049\n",
            "Train Epoch: 12 [23680/35339 (67%)]\tLoss: 0.092235\n",
            "Train Epoch: 12 [24320/35339 (69%)]\tLoss: 0.130887\n",
            "Train Epoch: 12 [24960/35339 (71%)]\tLoss: 0.074962\n",
            "Train Epoch: 12 [25600/35339 (72%)]\tLoss: 0.080335\n",
            "Train Epoch: 12 [26240/35339 (74%)]\tLoss: 0.064481\n",
            "Train Epoch: 12 [26880/35339 (76%)]\tLoss: 0.048303\n",
            "Train Epoch: 12 [27520/35339 (78%)]\tLoss: 0.039796\n",
            "Train Epoch: 12 [28160/35339 (80%)]\tLoss: 0.070691\n",
            "Train Epoch: 12 [28800/35339 (81%)]\tLoss: 0.023334\n",
            "Train Epoch: 12 [29440/35339 (83%)]\tLoss: 0.041382\n",
            "Train Epoch: 12 [30080/35339 (85%)]\tLoss: 0.104661\n",
            "Train Epoch: 12 [30720/35339 (87%)]\tLoss: 0.049776\n",
            "Train Epoch: 12 [31360/35339 (89%)]\tLoss: 0.048073\n",
            "Train Epoch: 12 [32000/35339 (90%)]\tLoss: 0.084727\n",
            "Train Epoch: 12 [32640/35339 (92%)]\tLoss: 0.074093\n",
            "Train Epoch: 12 [33280/35339 (94%)]\tLoss: 0.091547\n",
            "Train Epoch: 12 [33920/35339 (96%)]\tLoss: 0.054027\n",
            "Train Epoch: 12 [34560/35339 (98%)]\tLoss: 0.012432\n",
            "Train Epoch: 12 [35200/35339 (99%)]\tLoss: 0.101379\n",
            "Training accuracy: 98.03616333007812\n",
            "\n",
            "Validation set: Average loss: 0.2198, Accuracy: 3640/3870 (94%)\n",
            "\n",
            "\n",
            "Saved model to model_12.pth.\n",
            "Train Epoch: 13 [0/35339 (0%)]\tLoss: 0.019059\n",
            "Train Epoch: 13 [640/35339 (2%)]\tLoss: 0.027577\n",
            "Train Epoch: 13 [1280/35339 (4%)]\tLoss: 0.147123\n",
            "Train Epoch: 13 [1920/35339 (5%)]\tLoss: 0.043750\n",
            "Train Epoch: 13 [2560/35339 (7%)]\tLoss: 0.044203\n",
            "Train Epoch: 13 [3200/35339 (9%)]\tLoss: 0.017752\n",
            "Train Epoch: 13 [3840/35339 (11%)]\tLoss: 0.040145\n",
            "Train Epoch: 13 [4480/35339 (13%)]\tLoss: 0.088807\n",
            "Train Epoch: 13 [5120/35339 (14%)]\tLoss: 0.047699\n",
            "Train Epoch: 13 [5760/35339 (16%)]\tLoss: 0.069307\n",
            "Train Epoch: 13 [6400/35339 (18%)]\tLoss: 0.088545\n",
            "Train Epoch: 13 [7040/35339 (20%)]\tLoss: 0.050625\n",
            "Train Epoch: 13 [7680/35339 (22%)]\tLoss: 0.053961\n",
            "Train Epoch: 13 [8320/35339 (24%)]\tLoss: 0.037310\n",
            "Train Epoch: 13 [8960/35339 (25%)]\tLoss: 0.048716\n",
            "Train Epoch: 13 [9600/35339 (27%)]\tLoss: 0.068067\n",
            "Train Epoch: 13 [10240/35339 (29%)]\tLoss: 0.081531\n",
            "Train Epoch: 13 [10880/35339 (31%)]\tLoss: 0.076189\n",
            "Train Epoch: 13 [11520/35339 (33%)]\tLoss: 0.065571\n",
            "Train Epoch: 13 [12160/35339 (34%)]\tLoss: 0.058869\n",
            "Train Epoch: 13 [12800/35339 (36%)]\tLoss: 0.154172\n",
            "Train Epoch: 13 [13440/35339 (38%)]\tLoss: 0.135982\n",
            "Train Epoch: 13 [14080/35339 (40%)]\tLoss: 0.020608\n",
            "Train Epoch: 13 [14720/35339 (42%)]\tLoss: 0.048335\n",
            "Train Epoch: 13 [15360/35339 (43%)]\tLoss: 0.059091\n",
            "Train Epoch: 13 [16000/35339 (45%)]\tLoss: 0.040360\n",
            "Train Epoch: 13 [16640/35339 (47%)]\tLoss: 0.029834\n",
            "Train Epoch: 13 [17280/35339 (49%)]\tLoss: 0.036382\n",
            "Train Epoch: 13 [17920/35339 (51%)]\tLoss: 0.093266\n",
            "Train Epoch: 13 [18560/35339 (52%)]\tLoss: 0.072706\n",
            "Train Epoch: 13 [19200/35339 (54%)]\tLoss: 0.071446\n",
            "Train Epoch: 13 [19840/35339 (56%)]\tLoss: 0.060409\n",
            "Train Epoch: 13 [20480/35339 (58%)]\tLoss: 0.084760\n",
            "Train Epoch: 13 [21120/35339 (60%)]\tLoss: 0.141960\n",
            "Train Epoch: 13 [21760/35339 (61%)]\tLoss: 0.023827\n",
            "Train Epoch: 13 [22400/35339 (63%)]\tLoss: 0.021341\n",
            "Train Epoch: 13 [23040/35339 (65%)]\tLoss: 0.096048\n",
            "Train Epoch: 13 [23680/35339 (67%)]\tLoss: 0.048108\n",
            "Train Epoch: 13 [24320/35339 (69%)]\tLoss: 0.052048\n",
            "Train Epoch: 13 [24960/35339 (71%)]\tLoss: 0.041768\n",
            "Train Epoch: 13 [25600/35339 (72%)]\tLoss: 0.047992\n",
            "Train Epoch: 13 [26240/35339 (74%)]\tLoss: 0.048570\n",
            "Train Epoch: 13 [26880/35339 (76%)]\tLoss: 0.087944\n",
            "Train Epoch: 13 [27520/35339 (78%)]\tLoss: 0.060392\n",
            "Train Epoch: 13 [28160/35339 (80%)]\tLoss: 0.041412\n",
            "Train Epoch: 13 [28800/35339 (81%)]\tLoss: 0.016019\n",
            "Train Epoch: 13 [29440/35339 (83%)]\tLoss: 0.096893\n",
            "Train Epoch: 13 [30080/35339 (85%)]\tLoss: 0.027710\n",
            "Train Epoch: 13 [30720/35339 (87%)]\tLoss: 0.069292\n",
            "Train Epoch: 13 [31360/35339 (89%)]\tLoss: 0.032801\n",
            "Train Epoch: 13 [32000/35339 (90%)]\tLoss: 0.058585\n",
            "Train Epoch: 13 [32640/35339 (92%)]\tLoss: 0.100583\n",
            "Train Epoch: 13 [33280/35339 (94%)]\tLoss: 0.012207\n",
            "Train Epoch: 13 [33920/35339 (96%)]\tLoss: 0.057289\n",
            "Train Epoch: 13 [34560/35339 (98%)]\tLoss: 0.047390\n",
            "Train Epoch: 13 [35200/35339 (99%)]\tLoss: 0.135979\n",
            "Training accuracy: 98.16067504882812\n",
            "\n",
            "Validation set: Average loss: 0.2022, Accuracy: 3663/3870 (95%)\n",
            "\n",
            "\n",
            "Saved model to model_13.pth.\n",
            "Train Epoch: 14 [0/35339 (0%)]\tLoss: 0.061763\n",
            "Train Epoch: 14 [640/35339 (2%)]\tLoss: 0.025685\n",
            "Train Epoch: 14 [1280/35339 (4%)]\tLoss: 0.039239\n",
            "Train Epoch: 14 [1920/35339 (5%)]\tLoss: 0.084813\n",
            "Train Epoch: 14 [2560/35339 (7%)]\tLoss: 0.105155\n",
            "Train Epoch: 14 [3200/35339 (9%)]\tLoss: 0.043780\n",
            "Train Epoch: 14 [3840/35339 (11%)]\tLoss: 0.070640\n",
            "Train Epoch: 14 [4480/35339 (13%)]\tLoss: 0.099852\n",
            "Train Epoch: 14 [5120/35339 (14%)]\tLoss: 0.040661\n",
            "Train Epoch: 14 [5760/35339 (16%)]\tLoss: 0.036918\n",
            "Train Epoch: 14 [6400/35339 (18%)]\tLoss: 0.050218\n",
            "Train Epoch: 14 [7040/35339 (20%)]\tLoss: 0.078949\n",
            "Train Epoch: 14 [7680/35339 (22%)]\tLoss: 0.174287\n",
            "Train Epoch: 14 [8320/35339 (24%)]\tLoss: 0.035379\n",
            "Train Epoch: 14 [8960/35339 (25%)]\tLoss: 0.080344\n",
            "Train Epoch: 14 [9600/35339 (27%)]\tLoss: 0.139442\n",
            "Train Epoch: 14 [10240/35339 (29%)]\tLoss: 0.012306\n",
            "Train Epoch: 14 [10880/35339 (31%)]\tLoss: 0.036560\n",
            "Train Epoch: 14 [11520/35339 (33%)]\tLoss: 0.060172\n",
            "Train Epoch: 14 [12160/35339 (34%)]\tLoss: 0.059427\n",
            "Train Epoch: 14 [12800/35339 (36%)]\tLoss: 0.147295\n",
            "Train Epoch: 14 [13440/35339 (38%)]\tLoss: 0.086557\n",
            "Train Epoch: 14 [14080/35339 (40%)]\tLoss: 0.029773\n",
            "Train Epoch: 14 [14720/35339 (42%)]\tLoss: 0.018789\n",
            "Train Epoch: 14 [15360/35339 (43%)]\tLoss: 0.089324\n",
            "Train Epoch: 14 [16000/35339 (45%)]\tLoss: 0.041917\n",
            "Train Epoch: 14 [16640/35339 (47%)]\tLoss: 0.059399\n",
            "Train Epoch: 14 [17280/35339 (49%)]\tLoss: 0.028588\n",
            "Train Epoch: 14 [17920/35339 (51%)]\tLoss: 0.040115\n",
            "Train Epoch: 14 [18560/35339 (52%)]\tLoss: 0.023623\n",
            "Train Epoch: 14 [19200/35339 (54%)]\tLoss: 0.014207\n",
            "Train Epoch: 14 [19840/35339 (56%)]\tLoss: 0.045139\n",
            "Train Epoch: 14 [20480/35339 (58%)]\tLoss: 0.043468\n",
            "Train Epoch: 14 [21120/35339 (60%)]\tLoss: 0.015636\n",
            "Train Epoch: 14 [21760/35339 (61%)]\tLoss: 0.022214\n",
            "Train Epoch: 14 [22400/35339 (63%)]\tLoss: 0.075799\n",
            "Train Epoch: 14 [23040/35339 (65%)]\tLoss: 0.028105\n",
            "Train Epoch: 14 [23680/35339 (67%)]\tLoss: 0.119036\n",
            "Train Epoch: 14 [24320/35339 (69%)]\tLoss: 0.094760\n",
            "Train Epoch: 14 [24960/35339 (71%)]\tLoss: 0.093257\n",
            "Train Epoch: 14 [25600/35339 (72%)]\tLoss: 0.035255\n",
            "Train Epoch: 14 [26240/35339 (74%)]\tLoss: 0.143843\n",
            "Train Epoch: 14 [26880/35339 (76%)]\tLoss: 0.120801\n",
            "Train Epoch: 14 [27520/35339 (78%)]\tLoss: 0.080406\n",
            "Train Epoch: 14 [28160/35339 (80%)]\tLoss: 0.130078\n",
            "Train Epoch: 14 [28800/35339 (81%)]\tLoss: 0.059985\n",
            "Train Epoch: 14 [29440/35339 (83%)]\tLoss: 0.070720\n",
            "Train Epoch: 14 [30080/35339 (85%)]\tLoss: 0.016335\n",
            "Train Epoch: 14 [30720/35339 (87%)]\tLoss: 0.028136\n",
            "Train Epoch: 14 [31360/35339 (89%)]\tLoss: 0.041678\n",
            "Train Epoch: 14 [32000/35339 (90%)]\tLoss: 0.121792\n",
            "Train Epoch: 14 [32640/35339 (92%)]\tLoss: 0.083329\n",
            "Train Epoch: 14 [33280/35339 (94%)]\tLoss: 0.049805\n",
            "Train Epoch: 14 [33920/35339 (96%)]\tLoss: 0.106226\n",
            "Train Epoch: 14 [34560/35339 (98%)]\tLoss: 0.142907\n",
            "Train Epoch: 14 [35200/35339 (99%)]\tLoss: 0.039654\n",
            "Training accuracy: 98.21726989746094\n",
            "\n",
            "Validation set: Average loss: 0.2088, Accuracy: 3676/3870 (95%)\n",
            "\n",
            "\n",
            "Saved model to model_14.pth.\n",
            "Train Epoch: 15 [0/35339 (0%)]\tLoss: 0.074384\n",
            "Train Epoch: 15 [640/35339 (2%)]\tLoss: 0.035959\n",
            "Train Epoch: 15 [1280/35339 (4%)]\tLoss: 0.032902\n",
            "Train Epoch: 15 [1920/35339 (5%)]\tLoss: 0.019056\n",
            "Train Epoch: 15 [2560/35339 (7%)]\tLoss: 0.036001\n",
            "Train Epoch: 15 [3200/35339 (9%)]\tLoss: 0.056593\n",
            "Train Epoch: 15 [3840/35339 (11%)]\tLoss: 0.025957\n",
            "Train Epoch: 15 [4480/35339 (13%)]\tLoss: 0.031486\n",
            "Train Epoch: 15 [5120/35339 (14%)]\tLoss: 0.038380\n",
            "Train Epoch: 15 [5760/35339 (16%)]\tLoss: 0.022624\n",
            "Train Epoch: 15 [6400/35339 (18%)]\tLoss: 0.024488\n",
            "Train Epoch: 15 [7040/35339 (20%)]\tLoss: 0.065502\n",
            "Train Epoch: 15 [7680/35339 (22%)]\tLoss: 0.052227\n",
            "Train Epoch: 15 [8320/35339 (24%)]\tLoss: 0.063247\n",
            "Train Epoch: 15 [8960/35339 (25%)]\tLoss: 0.079929\n",
            "Train Epoch: 15 [9600/35339 (27%)]\tLoss: 0.049929\n",
            "Train Epoch: 15 [10240/35339 (29%)]\tLoss: 0.057764\n",
            "Train Epoch: 15 [10880/35339 (31%)]\tLoss: 0.028501\n",
            "Train Epoch: 15 [11520/35339 (33%)]\tLoss: 0.077904\n",
            "Train Epoch: 15 [12160/35339 (34%)]\tLoss: 0.057425\n",
            "Train Epoch: 15 [12800/35339 (36%)]\tLoss: 0.084093\n",
            "Train Epoch: 15 [13440/35339 (38%)]\tLoss: 0.069028\n",
            "Train Epoch: 15 [14080/35339 (40%)]\tLoss: 0.067967\n",
            "Train Epoch: 15 [14720/35339 (42%)]\tLoss: 0.052845\n",
            "Train Epoch: 15 [15360/35339 (43%)]\tLoss: 0.108957\n",
            "Train Epoch: 15 [16000/35339 (45%)]\tLoss: 0.013286\n",
            "Train Epoch: 15 [16640/35339 (47%)]\tLoss: 0.014671\n",
            "Train Epoch: 15 [17280/35339 (49%)]\tLoss: 0.067066\n",
            "Train Epoch: 15 [17920/35339 (51%)]\tLoss: 0.090264\n",
            "Train Epoch: 15 [18560/35339 (52%)]\tLoss: 0.031840\n",
            "Train Epoch: 15 [19200/35339 (54%)]\tLoss: 0.089797\n",
            "Train Epoch: 15 [19840/35339 (56%)]\tLoss: 0.043124\n",
            "Train Epoch: 15 [20480/35339 (58%)]\tLoss: 0.036651\n",
            "Train Epoch: 15 [21120/35339 (60%)]\tLoss: 0.041947\n",
            "Train Epoch: 15 [21760/35339 (61%)]\tLoss: 0.049571\n",
            "Train Epoch: 15 [22400/35339 (63%)]\tLoss: 0.061132\n",
            "Train Epoch: 15 [23040/35339 (65%)]\tLoss: 0.100376\n",
            "Train Epoch: 15 [23680/35339 (67%)]\tLoss: 0.024443\n",
            "Train Epoch: 15 [24320/35339 (69%)]\tLoss: 0.049015\n",
            "Train Epoch: 15 [24960/35339 (71%)]\tLoss: 0.060915\n",
            "Train Epoch: 15 [25600/35339 (72%)]\tLoss: 0.035705\n",
            "Train Epoch: 15 [26240/35339 (74%)]\tLoss: 0.055857\n",
            "Train Epoch: 15 [26880/35339 (76%)]\tLoss: 0.009836\n",
            "Train Epoch: 15 [27520/35339 (78%)]\tLoss: 0.064046\n",
            "Train Epoch: 15 [28160/35339 (80%)]\tLoss: 0.018630\n",
            "Train Epoch: 15 [28800/35339 (81%)]\tLoss: 0.032374\n",
            "Train Epoch: 15 [29440/35339 (83%)]\tLoss: 0.063615\n",
            "Train Epoch: 15 [30080/35339 (85%)]\tLoss: 0.120910\n",
            "Train Epoch: 15 [30720/35339 (87%)]\tLoss: 0.033805\n",
            "Train Epoch: 15 [31360/35339 (89%)]\tLoss: 0.100474\n",
            "Train Epoch: 15 [32000/35339 (90%)]\tLoss: 0.053080\n",
            "Train Epoch: 15 [32640/35339 (92%)]\tLoss: 0.091383\n",
            "Train Epoch: 15 [33280/35339 (94%)]\tLoss: 0.037941\n",
            "Train Epoch: 15 [33920/35339 (96%)]\tLoss: 0.122259\n",
            "Train Epoch: 15 [34560/35339 (98%)]\tLoss: 0.108629\n",
            "Train Epoch: 15 [35200/35339 (99%)]\tLoss: 0.040052\n",
            "Training accuracy: 98.38705444335938\n",
            "\n",
            "Validation set: Average loss: 0.1663, Accuracy: 3690/3870 (95%)\n",
            "\n",
            "\n",
            "Saved model to model_15.pth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "bseos0zQYylb",
        "outputId": "606635d8-21c1-481f-e9ce-89fb4cac9f6d"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "x = train_accuracy_points\n",
        "y = list(range(1, epochs + 1))\n",
        "plt.xlabel(\"Training Accuracy (%)\")\n",
        "plt.ylabel(\"Epochs\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVN1f1p7w59X"
      },
      "source": [
        "# Evaluate and Submit to Kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BM5qP64w5zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7848eb-51f1-42a1-8c9b-a991b8ea2c1b"
      },
      "source": [
        "\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "outfile = 'gtsrb_kaggle.csv'\n",
        "\n",
        "output_file = open(outfile, \"w\")\n",
        "dataframe_dict = {\"Filename\" : [], \"ClassId\": []}\n",
        "\n",
        "test_data = torch.load('testing/test.pt')\n",
        "file_ids = pickle.load(open('testing/file_ids.pkl', 'rb'))\n",
        "model = Net() # TODO: load your model here, don't forget to put it on Eval mode !\n",
        "model.load_state_dict(torch.load(\"model_15.pth\"))\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(test_data):\n",
        "    data = data.unsqueeze(0)\n",
        "    output = model(data)\n",
        "    pred = output.data.max(1, keepdim=True)[1].item()\n",
        "    file_id = file_ids[i][0:5]\n",
        "    dataframe_dict['Filename'].append(file_id)\n",
        "    dataframe_dict['ClassId'].append(pred)\n",
        "\n",
        "df = pd.DataFrame(data=dataframe_dict)\n",
        "df.to_csv(outfile, index=False)\n",
        "print(\"Written to csv file {}\".format(outfile))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written to csv file gtsrb_kaggle.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhSl_4kn6sox"
      },
      "source": [
        "# Submitting to Kaggle\n",
        "\n",
        "Now take this csv file, download it from your Google drive and then submit it to Kaggle to check performance of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05AyFu1SiKQs"
      },
      "source": [
        "# Summary of research\n",
        "In order to get a decent starting performance, I looked up some articles regarding construction and optimization of CNNs. The general intuition I developed for building CNNs was that:\n",
        "- Convolutions help capture and focus on spatial dependencies of a image, unlike normal neural networks\n",
        "- Pooling is a convenient, yet, effective tool for dimensionality reduction and in general, max-pooling works better than average-pooling.\n",
        "\n",
        "## Initial Performance\n",
        "Based on the above assumptions and a rudimentary idea of a convnet, the initial network achieved ~65% accuracy on validation set. Given the low accuracy, and based on the multiple layers of convolutions often observed on popular and highly performant networks like VGGNet and AlexNet, I made the following changes\n",
        "- Added another convolution with max-pooling\n",
        "- Increased out-channels on each convolution layer\n",
        "- Increased features on FC layers\n",
        "\n",
        "## Improved Performance\n",
        "I observed a significant jump to ~88% accuracy on the validation set, althought the training accuracy seemed much higher, indicating overfitting. I also noticed that the training speed was too low, so based on my read-ups regarding batch size and learning rate, I decided to:\n",
        "- Switching from SGD to Adam optimizer to handle overshooting due to high learning rate.\n",
        "- Switching over to cross entropy loss.\n",
        "- Increase batch size from 32 to 64 to improve generalization (thereby mitigating overfitting) and increase training speed.\n",
        "- Decreased learning rate from 0.01 to 0.0001\n",
        "- Increase Epochs from 10 to 15.\n",
        "\n",
        "## Final Performance and Analysis\n",
        "After the abovementioned changes, I was able to achieve ~97% accuracy on the test data. However, I noticed that there was a significant gap between training and validation accuracy (average of ~5%). In order to push the performance further and reduce this gap, I:\n",
        "- Attempted to add dropout to another convolution layer in order to increase generalization.\n",
        "- Added more FC layers to increase depth.\n",
        "- Reduced batch size to 32 and increased learning rate to counter possible variance.\n",
        "\n",
        "## Future Ideas\n",
        "- Image augmentation.\n",
        "- Regularization using weight decay\n",
        "- CutMix\n",
        "- KFold Cross Validation\n",
        "\n",
        "## References\n",
        "- [Comprehensive Guide to CNN](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
        "- https://medium.com/@dipti.rohan.pawar/improving-performance-of-convolutional-neural-network-2ecfe0207de7\n",
        "- [CNN Architectures](https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5)"
      ]
    }
  ]
}