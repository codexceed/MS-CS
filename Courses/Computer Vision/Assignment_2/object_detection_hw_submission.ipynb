{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwreVPXZIzZB"
   },
   "source": [
    "# Object Detection\n",
    " \n",
    "Hello, this is a very minimal notebook showing a working object detection network using just ~300 lines of modelling and training code. In this lab you will code an object detection with functionality similiar to a Single Shot Detector or YOLO detector. \n",
    " \n",
    "The first steps would be to understand what anchors are. Anchors are nothing but crops of a specific size and position in the image. The job of our object detector is to classify each anchor as containing an object or not. If the anchor contains an object, we want to find out by how much our anchor needs to change in shape and position to best fit the object in it. (By how much the height, width and center x and y cordinates change.)\n",
    " \n",
    "Hence, object detection is a multi task network which does classification and regression.\n",
    " \n",
    " \n",
    "The final objective of completing this notebook is to generate some visualizations of the network running on images in the validation set. In addition to this, please upload the weights of your network and share the google drive link. In this notebook below:\n",
    " \n",
    "**Fill this out with the Google drive link containing model weights**\n",
    "\n",
    "[Model State Dict (weights)](https://drive.google.com/file/d/1mkImfC5TqoVAyROv4qZyzlsoA7xErTzV/view?usp=sharing)\n",
    "...\n",
    " \n",
    "You should upload the completed notebook to NYU Brightspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gqANBiQI9hr",
    "outputId": "c000d3da-8566-4533-e9d9-8a45301da08a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'objectdetection-' already exists and is not an empty directory.\n",
      "/Users/sarthak/Repos/MS-CS/Courses/Computer Vision/Assignment_2/objectdetection-\n"
     ]
    }
   ],
   "source": [
    "# load important modules\n",
    "!git clone https://github.com/karanchahal/objectdetection-\n",
    "%cd objectdetection-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ak37idDZIzZC"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/sarthak/Repos/MS-CS/Courses/Computer Vision/cv_env/lib/python3.8/site-packages/PIL/_imaging.cpython-38-darwin.so, 0x0002): tried: '/Users/sarthak/Repos/MS-CS/Courses/Computer Vision/cv_env/lib/python3.8/site-packages/PIL/_imaging.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/usr/local/lib/_imaging.cpython-38-darwin.so' (no such file), '/usr/lib/_imaging.cpython-38-darwin.so' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sf/dnhwpw5j4gdfml0m9b2sgyb00000gn/T/ipykernel_8173/2391853472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshapes_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_shapes_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/MS-CS/Courses/Computer Vision/Assignment_2/objectdetection-/extras/shapes_loader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Repos/MS-CS/Courses/Computer Vision/cv_env/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# and should be considered private and subject to change.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PILLOW_VERSION\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/sarthak/Repos/MS-CS/Courses/Computer Vision/cv_env/lib/python3.8/site-packages/PIL/_imaging.cpython-38-darwin.so, 0x0002): tried: '/Users/sarthak/Repos/MS-CS/Courses/Computer Vision/cv_env/lib/python3.8/site-packages/PIL/_imaging.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64e')), '/usr/local/lib/_imaging.cpython-38-darwin.so' (no such file), '/usr/lib/_imaging.cpython-38-darwin.so' (no such file)"
     ]
    }
   ],
   "source": [
    "from extras.shapes_loader import get_shapes_loader\n",
    "from extras.util import *\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch.nn as nn\n",
    "from extras.encoder import ResnetEncoder\n",
    "import torch.nn.functional as F\n",
    "from extras.anchors import get_offsets\n",
    "from extras.boxes import box_iou, nms\n",
    " \n",
    "trainloader, valloader = get_shapes_loader(batch_sz=4)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT-JnR9WIzZG"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "First let's introduce our dataset. We have a Pytorch dataloader which outputs images and the ground truth data for the bounding boxes. We visualize the dataset below. You dont need to write any code here. However, you should dig into the fun ction that are called to better understand the structure of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdSVb1pYIzZJ"
   },
   "outputs": [],
   "source": [
    "sample, target = iter(trainloader).next()\n",
    "sample = torch.stack(sample,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "VztoWXFtIzZL",
    "outputId": "093e4140-2359-49b6-ce09-63fec847141e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "57B5bLG-IzZN",
    "outputId": "9562bebd-3186-4271-baf3-a98063ef6146",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visDet(sample, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utFxVXjqIzZQ"
   },
   "source": [
    "### Target Data for the Dataset\n",
    "\n",
    "The labels/targets of the dataset have 2 types of data for each image. \n",
    "\n",
    "1. The bounding boxes which is a tensor of size (N x 4).\n",
    "here is N is the number of objects in the image and 4 correspond to:\n",
    "\n",
    "    a. top left x coordinate\n",
    "    b. top left y coordinate\n",
    "    c. bottom right x coordinate\n",
    "    d. bottom right y coordinate\n",
    "\n",
    "2. The second type of targets are the classification labels of size Nx1. Here N corresponds to the number of objects in the image and the 1 corresponds to the label of the object (whether it is a triangle, square or circle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHGsaaHMIzZQ",
    "outputId": "5668b87e-2925-480b-ac30-048470ba8776",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(target[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq87yw2bIzZT"
   },
   "source": [
    "### Converting Target Data to Ground Truth \n",
    "\n",
    "Here we assign bounding boxes to the various anchor boxes in our image. Anchor boxes are rectangular chunks in the image and are spread all across the image. In object detection, we divide the image into a large number of chunks of different sizes and specify two characteristics of that chunk:\n",
    "\n",
    "1. Whether it has an object in it or not. This is done by measuring IOU of the chunk with the bounding boxes. If the IOU is > 0.7, we say that the chunk contains the object. If it is less than 0.3, we say the chunk only contains background. If the value is somewhere in between, we say it is a bad chunk and do not use it's contribution to the loss in our backpropogation.\n",
    "\n",
    "2. How much should the chunk change in height, width and how much should it translate to best fit the object. These offsets are 4 values, for the height, width, center x and y coordinate. You should use measurements relative to the default anchor position/scale (see below for more details on the anchors).\n",
    "\n",
    "\n",
    "Anchor boxes might be explained best by this image:\n",
    "![Anchor Boxes](https://lilianweng.github.io/lil-log/assets/images/SSD-framework.png)\n",
    "\n",
    "Now, first we create the chunks or \"anchors\" for our image. Since our image is 128x 128 in size. In this lab we want to use a stride of 16.  Hence, we should get 192 anchors.\n",
    "\n",
    "\n",
    "### Why do we get the size 192 ?\n",
    "\n",
    "We get the size of 192 as we have a total of 192 anchors in an image of 128 x 128 while using a stride of 16 and we have 3 anchors in each position of the image. \n",
    "\n",
    "Let's break this down.\n",
    "\n",
    "128/16 = 8.\n",
    "\n",
    "Hence, our neural network should output a feature map of 8 x 8 in spatial dimensions. Each grid in this feature map is an anchor position. Each position has 3 boxes/anchors with default shapes -> size 40x40, 50x50 and 60x60, centered at each position.\n",
    "\n",
    "Hence the total anchors are 8x8x3 = 192. \n",
    "\n",
    "Your first job is to write the code that creates these 192 anchors from an image of size 128x128. Each anchor is represented by the top left and bottom right x,y coordinates of the anchors in regards with a 128x128 image. \n",
    "\n",
    "Hence, the output of the below function should be a tensor of size [192,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvcyW6AkIzZZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gt_boxes():\n",
    "  \"\"\"\n",
    "  Generate 192 boxes where each box is represented by :\n",
    "  [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n",
    "\n",
    "  Each anchor position should generate 3 boxes according to the scales and ratios given.\n",
    "\n",
    "  Return this result as a numpy array of size [192,4]\n",
    "  \"\"\"\n",
    "  stride = 16 # The stride of the final feature map is 16 (the model compresses the image from 128 x 128 to 8 x 8)\n",
    "  map_sz = 128 # this is the length of height/width of the image\n",
    "\n",
    "  scales = torch.tensor([40,50,60])\n",
    "  ratios = torch.tensor([[1,1]]).view(1,2)\n",
    "  img = sample[0][0]  # Get a single image\n",
    "\n",
    "  gt_boxes = []\n",
    "  for i in range(0, map_sz, stride):\n",
    "    for j in range(0, map_sz, stride):\n",
    "      center_x, center_y = i + stride // 2, j + stride // 2\n",
    "      for k in scales[:]:\n",
    "        offset = int(k) // 2\n",
    "        x_top_left, y_top_left, x_bot_right, y_bot_right = center_x - offset, center_y - offset, center_x + offset, center_y + offset\n",
    "        gt_boxes.append([x_top_left, y_top_left, x_bot_right, y_bot_right])\n",
    "\n",
    "  gt_boxes = torch.Tensor(gt_boxes)\n",
    "  return gt_boxes\n",
    "\n",
    "gt_boxes = get_gt_boxes().to(device)\n",
    "assert gt_boxes.size() == (192,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRmSTATwIzZZ"
   },
   "source": [
    "# A Pictoral Representation of our Model\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*7heX-no7cdqllky-GwGBfQ.png)\n",
    "\n",
    "\n",
    "However instead of $2k$ in the given image we just have k scores. We group triangles, squares and circles into one foreground class for this demo. Of course, we can have seperate predictions for each class but that is added complexity and is not implemented here.\n",
    "\n",
    "\n",
    "Your next job is to construct the model. Most parts of the model is already constructed, your job is to add the classification and bounding box regression heads to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yt--F8HcIzZU"
   },
   "outputs": [],
   "source": [
    "class ShapesModel(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(ShapesModel, self).__init__()\n",
    "\n",
    "    # for each grid in the feature map we have 3 anchors of sizes: 40x40, 50x50, 60x60\n",
    "    num_anchors = 3\n",
    "\n",
    "    # regular resnet 18 encoder\n",
    "    self.encoder = ResnetEncoder(num_layers=18, pretrained=False)\n",
    "    self.sigm = nn.Sigmoid()\n",
    "\n",
    "    # a small conv net\n",
    "    self.conv = nn.Conv2d(\n",
    "        256, 256, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "\n",
    "    # TODO: Add a Convolutional Layer to predict the class predictions. This is a head that predicts whether a chunk/anchor contains an object or not.\n",
    "    self.cls_logits = nn.Conv2d(\n",
    "        256, 3, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "\n",
    "    # TODO: Add a Convolutional Layer to predict the class predictions. This is a  a head that regresses over the 4 bounding box offsets for each anchor\n",
    "    self.bbox_pred = nn.Conv2d(\n",
    "        256, 12, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "\n",
    "  def permute_and_flatten(self, layer, N, A, C, H, W):\n",
    "    # helper function that rearranges the input for the loss function\n",
    "    layer = layer.view(N, -1, C, H, W)\n",
    "    layer = layer.permute(0, 3, 4, 1, 2)\n",
    "    layer = layer.reshape(N, -1, C)  # 1x192x4\n",
    "    return layer\n",
    "\n",
    "  def get_predict_regressions(self, cls_pred, box_pred):\n",
    "    # helper function that gets outputs in the right shape for applying the loss\n",
    "    N, AxC, H, W = cls_pred.shape\n",
    "    Ax4 = box_pred.shape[1]\n",
    "    A = Ax4 // 4\n",
    "    C = AxC // A\n",
    "    cls_pred = self.permute_and_flatten(\n",
    "        cls_pred, N, A, C, H, W\n",
    "    )\n",
    "    \n",
    "    box_pred = self.permute_and_flatten(\n",
    "        box_pred, N, A, 4, H, W\n",
    "    )\n",
    "    return cls_pred, box_pred\n",
    "\n",
    "  def forward(self, x):\n",
    "    bt_sz = x.size(0)\n",
    "\n",
    "    # we take the 3rd output feature map of size 8 x 8 from\n",
    "    # the resnet18 encoder this means that the stride\n",
    "    # is 16 as our input image is 128x128 in size.\n",
    "    x = self.encoder(x)[3]\n",
    "\n",
    "    x = F.relu(self.conv(x))\n",
    "    \n",
    "\n",
    "    cls_pred = self.sigm(self.cls_logits(x))\n",
    "    box_pred = self.bbox_pred(x)\n",
    "\n",
    "    cls_pred, box_pred = self.get_predict_regressions(cls_pred, box_pred)\n",
    "\n",
    "    return cls_pred.squeeze(2), box_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-tnPWV9IzZX",
    "outputId": "3272fae7-f77b-4c3d-d8a2-e7368930bd04",
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = ShapesModel()\n",
    "a = torch.randn(1,3,128,128) # dummy input\n",
    "cls_pred, box_pred = m(a)\n",
    "print('The outputs of the net are of size: ', cls_pred.size(), box_pred.size())\n",
    "\n",
    "assert cls_pred.size() == (1,192) and box_pred.size() == (1,192,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNbDq_8xIzZb"
   },
   "source": [
    "Each anchor is represented by the top left and bottom right x,y coordinates of the anchors in regards with a 128x128 image. \n",
    "\n",
    "Next we have to assign a class label to each anchor and in case the anchor has an IOU > 0.7 with any real object assign 4 offsets to the anchor. We do this for every single image. Each image would have it's own set of anchors with different class probabilities and offsets as that depends on what objects are situated in that image. \n",
    "\n",
    "Your job is to calculate those values. Fill out the below function. Hint: Use the `box_iou` and `get_offsets` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMRwLlVqIzZc"
   },
   "outputs": [],
   "source": [
    "def get_bbox_gt(ex_boxes, gt_boxes, sz=128):\n",
    "  '''\n",
    "\n",
    "  INPUT:\n",
    "  ex_boxes: [Nx4]: Bounding boxes in the image. Here N is the number of bounding boxes the image has\n",
    "  gt_boxes: [192 x 4]: Anchor boxes of an image of size 128 x 128 with stride 16. \n",
    "  sz : 128\n",
    "  OUTPUT: \n",
    "  gt_classes: [192 x 1] : Class labels for each anchor: 1 is for foreground, 0 is for background and -1 is for a bad anchor. [where IOU is between 0.3 and 0.7]\n",
    "  gt_offsets: [192 x 4]: Offsets for anchor to best fit the bounding box object. 0 values for 0 and -1 class anchors.\n",
    "\n",
    "  '''\n",
    "  high_threshold = 0.7\n",
    "  low_threshold = 0.3\n",
    "\n",
    "  # Get maximum IOU for each bounding box and the corresponding index of anchor\n",
    "  bbox_iou = box_iou(ex_boxes, gt_boxes)\n",
    "  bbox_max_iou, indices = torch.max(bbox_iou, 0)\n",
    "\n",
    "  # Get labels corresponding to IOUs.\n",
    "  gt_classes = bbox_max_iou.view(192, 1)\n",
    "  gt_classes[gt_classes >= high_threshold] = 1\n",
    "  gt_classes[gt_classes <= low_threshold] = 0\n",
    "  gt_classes[(gt_classes < high_threshold) & (gt_classes > low_threshold)] = -1\n",
    "\n",
    "  # Get offsets for all anchors.\n",
    "  gt_offsets = get_offsets(gt_boxes, ex_boxes[indices, :])\n",
    "\n",
    "  # Set invalid anchor offsets to 0\n",
    "#   gt_offsets[(gt_classes < 1).reshape(192), :] = torch.tensor([0.0, 0.0, 0.0, 0.0]).to(device)\n",
    "\n",
    "  return gt_classes, gt_offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geYdBXveIzZe"
   },
   "outputs": [],
   "source": [
    "def get_targets(target, sample):\n",
    "    '''\n",
    "    Input\n",
    "    target => Set of bounding boxes for each image.\n",
    "    Sample => Each image\n",
    "    Output:\n",
    "    Bounding box offsets and class labels for each anchor.\n",
    "    '''\n",
    "\n",
    "    batched_preds = []\n",
    "    batched_offsets = []\n",
    "    final_cls_targets = []\n",
    "    final_box_offsets = []\n",
    "    for t, s in zip(target, sample):\n",
    "        bboxes = t['bounding_box'].to(device).float()\n",
    "        class_targets, box_offsets = get_bbox_gt(bboxes, gt_boxes, sz=128)\n",
    "        final_cls_targets.append(class_targets)\n",
    "        final_box_offsets.append(box_offsets)\n",
    "    \n",
    "    final_cls_targets = torch.stack(final_cls_targets, dim=0)\n",
    "    final_box_offsets = torch.stack(final_box_offsets, dim=0)\n",
    "\n",
    "\n",
    "    return final_cls_targets, final_box_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MaTWpH2IzZg",
    "outputId": "d812d4c8-fbac-444f-f86f-7253b0a4188a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample, target = iter(trainloader).next()\n",
    "sample = torch.stack(sample,dim=0).to(device)\n",
    "\n",
    "\n",
    "class_targets, box_targets = get_targets(target, sample)\n",
    "print(class_targets.size(), box_targets.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5S_E5BmIzZj"
   },
   "source": [
    "This is the meat of object detection right there ! The correct calculation of the anchors for an image and it's ground truth. Now that this is done, we can move on to our neural network training code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlcVJzGlIzZj"
   },
   "source": [
    "## Loss Function\n",
    "\n",
    "One more small kink in the puzzle of object detection is the way the classification loss is calculated. Here, the classification loss is the binary cross entropy loss. The loss is calculated only for those anchors which are background and foreground anchors NOT the negative ones. \n",
    "\n",
    "One more kink in this setup is that we should sample background and foreground anchors in the ratio of 3:1. This is because there are too many background anchors and the model will predict a background anchor for every anchor and still get a 90% accuracy on the classification. To prevent this, we do this sampling which is also called hard online negative sampling. However, as we have a very small amount of anchors here (192) this isn't a problem. However, for smaller strides and different scales and ratios for each position, thie number can balloon to upto 100,000. Hence, implementing hard online negative sampling is left to be optional to you.\n",
    "\n",
    "For the bounding box regressions, use a Smooth L1 loss, this works well and prevents outliers.\n",
    "\n",
    "Your next task is to write the code for the class_loss and bbox_loss. The classification loss should be applied to all positive (containing an object) and negative anchors (does not contain an object). The loss should not be calculated for the anchors whose IOU values are between 0.7 and 0.3.\n",
    "\n",
    "Apply a Smooth L1 loss to regress on the bounding box offsets, (only apply this loss for anchors which contain an object !)\n",
    "\n",
    "**Extra Credit**: Implement Hard Online Negative Sampling, however it is not necessasary. You can get this model to train without using it as the number of anchors are low for an image in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi13XEtKI6pj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def class_loss(out_pred, class_targets, device=\"cpu\"):\n",
    "    # TODO return class loss\n",
    "    bce = nn.BCELoss().to(device)\n",
    "    targets = class_targets.view(4, 192)\n",
    "    loss = torch.zeros(1).to(device)\n",
    "\n",
    "    # Calculate loss for each image separately\n",
    "    for i in range(out_pred.shape[0]):\n",
    "      pred, target, filter = out_pred[i], targets[i], targets[i] != -1\n",
    "\n",
    "      # Do not calculate loss for maps without any relevant values\n",
    "      if not torch.any(filter):\n",
    "        continue\n",
    "\n",
    "      loss += bce(pred[filter], target[filter])\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def bbox_loss(out_bbox, box_targets, class_targets, device=\"cpu\"):\n",
    "    # TODO return bounding box offset loss\n",
    "    sl1 = nn.SmoothL1Loss().to(device)\n",
    "    class_targets = class_targets.view(4, 192)\n",
    "    loss = torch.zeros(1).to(device)\n",
    "\n",
    "    # Calculate loss for each image separately\n",
    "    for i in range(out_bbox.shape[0]):\n",
    "      bbox, target, filter = out_bbox[i], box_targets[i], class_targets[i] == 1\n",
    "\n",
    "      # Do not calculate loss for maps without any relevant values\n",
    "      if not torch.any(filter):\n",
    "        continue\n",
    "\n",
    "      loss += sl1(bbox[filter, :], target[filter, :])\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u42yvSLCI6pl"
   },
   "source": [
    "## Training Function. \n",
    "\n",
    "This is the training function, you do not need to change any code here. You can add a validation function here to verify that your model is working well. However, you do not need to submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trximgTSIzZj"
   },
   "outputs": [],
   "source": [
    "def train(ep, model, trainloader, optimizer):\n",
    "        total_loss = 0\n",
    "        b_loss = 0\n",
    "        c_loss = 0\n",
    "        for i, (ims, targets) in enumerate(trainloader):\n",
    "            ims = torch.stack(list(ims), dim=0).to(device)\n",
    "            class_targets, box_targets = get_targets(targets, ims)\n",
    "            class_targets, box_targets = class_targets.to(device), box_targets.to(device)\n",
    "            out_pred, out_box = model(ims)\n",
    "            loss_cls = class_loss(out_pred, class_targets, device)\n",
    "            loss_bbox = bbox_loss(out_box, box_targets, class_targets, device)\n",
    "            loss = loss_cls + loss_bbox\n",
    "\n",
    "            if loss.item() != 0:\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            c_loss += loss_cls.item()\n",
    "            b_loss += loss_bbox.item()\n",
    "            \n",
    "        avg_c_loss = float(c_loss / len(trainloader))\n",
    "        avg_b_loss = float(b_loss / len(trainloader))\n",
    "        print('Trained Epoch: {} | Avg Classification Loss: {}, Bounding loss: {}'.format(ep, avg_c_loss, avg_b_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8f5IswpIzZl",
    "outputId": "4f3164a9-7665-4918-ae73-0335a6cfac7a"
   },
   "outputs": [],
   "source": [
    "model = ShapesModel().to(device)\n",
    "optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.1, momentum=0.9, weight_decay=1e-4)    \n",
    "for ep in range(25):\n",
    "    train(ep, model, trainloader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqX8k4qYIzZn"
   },
   "source": [
    "### Now let's visualize the predictions !\n",
    "\n",
    "The most exciting part of any neural network model setup. First let's visualize our ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRWPArY7IzZo"
   },
   "outputs": [],
   "source": [
    "sample, target = iter(valloader).next()\n",
    "sample = torch.stack(sample,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "GXpoJRadIzZq",
    "outputId": "1bc90e40-fe95-4a41-d744-529cc732a34c"
   },
   "outputs": [],
   "source": [
    "visDet(sample, target)\n",
    "print(target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAZPrnKcl6oU"
   },
   "source": [
    "### Our Model Predictions\n",
    "\n",
    "Note: We perform NMS upon our final predictions to clear up the output. This is done as a lot of anchors near to the actual object will fire resulting in a lot of boxes. NMS handles these multiple predictions by merging and giving a single box for a predicted object instead of multiple. This is best signified through another image:\n",
    "\n",
    "![NMS](https://miro.medium.com/max/1000/0*WI5_K3bAbYaRyzE-.png)\n",
    "\n",
    "Now onto our model predictions !\n",
    "\n",
    "Your final task is to write code to visualize the model predictions on an image in the validation set. You can use the `visDet` function to help out with visualizing. Your main job would be to select what anchors (after applying offsets) to draw on the image. Generally, if the probability is > 0.7. It's a good idea to use the anchor. Also, you can use the `nms` function inside `boxes.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z4H0o2tuIzZt",
    "outputId": "6091a693-4dd1-4e11-f332-4272a447433f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_coordinates(gt_boxes, offsets, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Return box corner coordinates for given top-left coordinates and offset values.\n",
    "\n",
    "    Args:\n",
    "      gt_boxes: Ground truth anchor box coordinates.\n",
    "      offsets: Tensor of box top-left corner and corresponding offsets.\n",
    "\n",
    "    Returns:\n",
    "      Tensor of coordinate pairs.\n",
    "    \"\"\"\n",
    "    # Inverse function of get_offsets()\n",
    "\n",
    "    gt_width = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
    "    gt_height = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
    "    gt_center_x = gt_boxes[:, 0] + 0.5*gt_width\n",
    "    gt_center_y = gt_boxes[:, 1] + 0.5*gt_height\n",
    "\n",
    "    img_boxes = []\n",
    "    for i in range(offsets.shape[0]):\n",
    "      delta_x, delta_y, delta_scaleX, delta_scaleY = offsets[i, :, 0], offsets[i, :, 1], offsets[i, :, 2], offsets[i, :, 3]\n",
    "      bbox_width = gt_width / torch.exp(delta_scaleX)\n",
    "      bbox_height = gt_height / torch.exp(delta_scaleY)\n",
    "\n",
    "      bbox_center_x = gt_center_x - delta_x * bbox_width\n",
    "      bbox_center_y = gt_center_y - delta_y * bbox_height\n",
    "\n",
    "      # Initialize empty coordinates matrix\n",
    "      bbox_coord = torch.zeros(192, 4).to(device)\n",
    "\n",
    "      bbox_coord[:, 0] = bbox_center_x - 0.5*bbox_width\n",
    "      bbox_coord[:, 1] = bbox_center_y - 0.5*bbox_height\n",
    "      bbox_coord[:, 2] = bbox_width + bbox_coord[:, 0]\n",
    "      bbox_coord[:, 3] = bbox_height + bbox_coord[: ,1]\n",
    "\n",
    "      # Append coordinates for this image\n",
    "      img_boxes.append(bbox_coord)\n",
    "    \n",
    "    # Stack coordinates for all images into a single tensor\n",
    "    img_boxes = torch.stack(img_boxes, dim=0)\n",
    "\n",
    "    return img_boxes\n",
    "\n",
    "\n",
    "def get_valid_boxes(box_labels, bboxes):\n",
    "  image_boxes = []\n",
    "  for i in range(box_labels.shape[0]):\n",
    "    box_dict = {}\n",
    "    \n",
    "    labels, coords = box_labels[i, :], bboxes[i, :]\n",
    "    filter = labels > 0.7\n",
    "    labels, coords = labels[filter], coords[filter, :]\n",
    "\n",
    "\n",
    "    box_dict[\"bounding_box\"] = coords.detach().cpu()\n",
    "    box_dict[\"labels\"] = labels.detach().cpu()\n",
    "    image_boxes.append(box_dict)\n",
    "    \n",
    "  for i in range(len(image_boxes)):\n",
    "    boxes = image_boxes[i][\"bounding_box\"]\n",
    "    scores = image_boxes[i][\"labels\"]\n",
    "    \n",
    "\n",
    "    if boxes.nelement() == 0:\n",
    "      continue\n",
    "    \n",
    "    keep_idx = nms(boxes, scores, 0.7)\n",
    "    scores = scores[keep_idx].view(-1, 1)\n",
    "    boxes = boxes[keep_idx].view(-1, 4)\n",
    "\n",
    "    image_boxes[i] = {\"bounding_box\": boxes, \"labels\": scores}\n",
    "  \n",
    "  return image_boxes\n",
    "\n",
    "\n",
    "\n",
    "def visPred(model, sample,):\n",
    "    #TODO: visualize your model predictions on the sample image.\n",
    "    sample = sample.to(device)\n",
    "    out_pred, out_box = model(sample)\n",
    "    bboxes = get_coordinates(gt_boxes, out_box, device)\n",
    "\n",
    "    valid_boxes = get_valid_boxes(out_pred, bboxes)\n",
    "\n",
    "\n",
    "    visDet(sample.cpu(), valid_boxes)\n",
    "\n",
    "model.eval()\n",
    "visPred(model, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../cv_assignment_2_obj_detection_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCNCE3PenkNa"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Not bad, right ? This is a very barebones implementation and trained for a very short amount of epochs. However this is the meat and bones of most anchor based state of the art detectors out today. We can improve accuracy by doing multiscale training by incorporating a Feature Pyramid Network but that is for you to discover :') \n",
    "\n",
    "## References\n",
    "\n",
    "Some foundational/good papers on object detection. This list is non exhaustive\n",
    "\n",
    "1. [Single Shot Detector](https://arxiv.org/abs/1512.02325)\n",
    "2. [YOLO](https://arxiv.org/abs/1612.08242)\n",
    "3. [Faster RCNN](https://arxiv.org/abs/1506.01497)\n",
    "4. [Feature Pyramid Network](https://arxiv.org/abs/1612.03144)\n",
    "5. Bonus Read: [DETR- State of the art Object detector based on Transformers](https://arxiv.org/abs/2005.12872)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUfCjZRrI6pw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "object_detection_hw_submission.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
